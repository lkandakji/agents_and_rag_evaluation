{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e537e271-4b08-491f-8cf7-c9be1f3fcf15",
   "metadata": {},
   "source": [
    "## Exact-Match Evaluation as a Baseline\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Exact match is one of the simplest evaluation metrics used in question–answering systems: a response is marked correct only if it matches the reference answer exactly. Despite its simplicity and its well-known limitations it remains a useful baseline in controlled settings. This notebook includes exact match not because it is sufficient, but because it provides a clear lower bound against which more flexible or semantic evaluation methods can be compared.\n",
    "\n",
    "In high-stakes or open-ended tasks, exact match is often too brittle to reflect meaningful correctness. However, its strictness can be informative when the task admits a narrow answer space, or when the goal is to detect surface-level failures such as hallucination, formatting errors, or deviation from required outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f58a85",
   "metadata": {},
   "source": [
    "### Experimental Setup\n",
    "We apply exact-match evaluation to a set of question–answer pairs with predefined reference answers. The metric assigns a binary score, marking responses as correct only when the generated output matches the reference string exactly after normalisation.\n",
    "\n",
    "This evaluation deliberately ignores semantic equivalence and paraphrasing. As such, it isolates a narrow class of failures related to precision and adherence to specification, rather than general understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950fae1e",
   "metadata": {},
   "source": [
    "### What this metric captures and what it misses\n",
    "\n",
    "Exact match is effective at detecting:\n",
    "- hallucinated content where a precise answer is required,\n",
    "- formatting or schema violations,\n",
    "- failure to follow explicit instructions.\n",
    "\n",
    "At the same time, it systematically underestimates performance in cases where multiple correct phrasings exist or where partial correctness is meaningful. For this reason, exact match should not be interpreted as a comprehensive measure of model quality, but as a diagnostic signal within a broader evaluation suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a25d7b-9f0a-4c2d-9b5b-5a1e8a9f3b1c",
   "metadata": {},
   "source": [
    "### Installing Dependencies\n",
    "\n",
    "This first code cell handles the installation of the necessary Python libraries. \n",
    "- `langchain`: The core library for building applications with LLMs.\n",
    "- `langchain_openai`: Provides specific integrations for using OpenAI's models within the LangChain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121dcc53-70ec-48df-adac-cbd424c66adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `%pip` command is used to install Python packages directly from a Jupyter cell.\n",
    "# The `-U` flag ensures that the packages are upgraded to their latest versions.\n",
    "# The `--quiet` flag suppresses the installation output for a cleaner notebook.\n",
    "# %pip install -U --quiet langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8b1a3-c5f1-4a3b-9b8a-1a8c7e9d8f7b",
   "metadata": {},
   "source": [
    "### Setting Up Environment Variables\n",
    "\n",
    "- **`LANGCHAIN_ENDPOINT`**: This tells LangChain where to send the logging and tracing data. We point it to the LangSmith API endpoint.\n",
    "- **`LANGCHAIN_API_KEY`**: This is your personal key to authenticate with your LangSmith account, allowing you to create datasets and log evaluation runs.\n",
    "- **`OPENAI_API_KEY`**: This is your key for the OpenAI API, which is required to make calls to models like `gpt-3.5-turbo`.\n",
    "\n",
    "You must replace the placeholder values (`\"YOUR API KEY\"` and `\"Your openai api key\"`) with your actual keys for this notebook to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f610c6e-144b-47c8-9791-eaf4f42a8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint as an environment variable.\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\" # Set your LangSmith API key as an environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your openai api key\" # Set your OpenAI API key as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62061d-0fb9-4ba9-b185-ff8c9746fb72",
   "metadata": {},
   "source": [
    "### Create an Evaluation Dataset\n",
    "\n",
    "- **Inputs**: The data that will be fed into your model (e.g., a user's prompt).\n",
    "- **Outputs (Reference Labels)**: The corresponding \"ground truth\" or expected answer that you want the model to produce.\n",
    "\n",
    "Here, we will create a small dataset named `\"Oracle of Exactness\"` directly in LangSmith. It will contain two examples designed to test for precise outputs. We first check if the dataset already exists to avoid creating duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8ca802-e306-4632-afec-e9d655c84982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith # Import the LangSmith client library.\n",
    "\n",
    "client = langsmith.Client() # Instantiate the LangSmith client to interact with the platform.\n",
    "dataset_name = \"Oracle of Exactness\" # Define a name for our new dataset.\n",
    "\n",
    "# Check if a dataset with this name already exists in your LangSmith project.\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    # If the dataset does not exist, create it.\n",
    "    ds = client.create_dataset(dataset_name)\n",
    "    # Add examples to the newly created dataset.\n",
    "    client.create_examples(\n",
    "        # 'inputs' is a list of dictionaries, each representing an input to the model.\n",
    "        inputs=[\n",
    "            {\n",
    "                \"prompt_template\": \"State the year of the declaration of independence. Respond with just the year in digits, nothign else\"\n",
    "            },\n",
    "            {\"prompt_template\": \"What's the average speed of an unladen swallow?\"},\n",
    "        ],\n",
    "        # 'outputs' is a list of dictionaries with the corresponding expected or ground-truth answers.\n",
    "        outputs=[{\"output\": \"1776\"}, {\"output\": \"5\"}],\n",
    "        # 'dataset_id' links these examples to the dataset we created above.\n",
    "        dataset_id=ds.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ea231-7901-44b8-9d66-761d3aca140a",
   "metadata": {},
   "source": [
    "### Define the System and Evaluators\n",
    "\n",
    "Now we'll set up the components needed to run the evaluation. This involves three key parts:\n",
    "\n",
    "1.  **The System Under Test (`predict_result`)**: This is the function that we want to evaluate. It takes an input dictionary (matching the structure of our dataset inputs), uses an OpenAI model to generate a response, and returns the result in a structured output dictionary.\n",
    "\n",
    "2.  **A Custom Evaluator (`compare_label`)**: While LangSmith provides a built-in `\"exact_match\"` evaluator, we define our own here to demonstrate how you can create custom evaluation logic. This function receives the model's output (`run`) and the ground truth data (`example`), compares them, and returns a structured `EvaluationResult`. The `@run_evaluator` decorator registers this function with LangSmith so it can be used in an evaluation run.\n",
    "\n",
    "3.  **The Evaluation Configuration (`RunEvalConfig`)**: This object bundles all the evaluators we want to apply to each model prediction. We include both LangSmith's pre-built `\"exact_match\"` evaluator and our custom `compare_label` function. This will allow us to see their results side-by-side and confirm they produce the same scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad3e9fc-72ac-4854-a67a-378ae0c8c91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'impressionable-crew-29' at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/4f23ec54-3cf8-44fc-a729-ce08ad855bfd/compare?selectedSessions=a0672ba4-e513-4fef-84b8-bab439581721\n",
      "\n",
      "View all tests for Dataset Oracle of Exactness at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/4f23ec54-3cf8-44fc-a729-ce08ad855bfd\n",
      "[------------------------------------------------->] 2/2"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.exact_match</th>\n",
       "      <th>feedback.matches_label</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2b4532af-445e-46aa-8170-d34c3af724a8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.545045</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.265404</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.357376</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.451211</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.545045</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.638880</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.732714</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.exact_match feedback.matches_label error  execution_time  \\\n",
       "count               2.000000                      2     0        2.000000   \n",
       "unique                   NaN                      2     0             NaN   \n",
       "top                      NaN                  False   NaN             NaN   \n",
       "freq                     NaN                      1   NaN             NaN   \n",
       "mean                0.500000                    NaN   NaN        0.545045   \n",
       "std                 0.707107                    NaN   NaN        0.265404   \n",
       "min                 0.000000                    NaN   NaN        0.357376   \n",
       "25%                 0.250000                    NaN   NaN        0.451211   \n",
       "50%                 0.500000                    NaN   NaN        0.545045   \n",
       "75%                 0.750000                    NaN   NaN        0.638880   \n",
       "max                 1.000000                    NaN   NaN        0.732714   \n",
       "\n",
       "                                      run_id  \n",
       "count                                      2  \n",
       "unique                                     2  \n",
       "top     2b4532af-445e-46aa-8170-d34c3af724a8  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'impressionable-crew-29',\n",
       " 'results': {'893730f0-393d-4c40-92f9-16ce24aaec1f': {'input': {'prompt_template': \"What's the average speed of an unladen swallow?\"},\n",
       "   'feedback': [EvaluationResult(key='exact_match', score=0, value=None, comment=None, correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('089a016a-d847-4a26-850c-afc0e78879d5'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='matches_label', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 0.732714,\n",
       "   'run_id': '2b4532af-445e-46aa-8170-d34c3af724a8',\n",
       "   'output': {'output': 'The average speed of an unladen European swallow is approximately 20.1 miles per hour (32.4 km/h).'},\n",
       "   'reference': {'output': '5'}},\n",
       "  'ec9d8754-d264-4cec-802e-0c33513843d8': {'input': {'prompt_template': 'State the year of the declaration of independence.Respond with just the year in digits, nothign else'},\n",
       "   'feedback': [EvaluationResult(key='exact_match', score=1, value=None, comment=None, correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cd4c7ede-f367-4d9c-b424-577bf054bf21'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='matches_label', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 0.357376,\n",
       "   'run_id': '82b65c5c-bfbf-4d2b-9c05-3bbd1cd4e711',\n",
       "   'output': {'output': '1776'},\n",
       "   'reference': {'output': '1776'}}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig # Import the configuration class for evaluation runs.\n",
    "from langchain_openai import ChatOpenAI # Import the ChatOpenAI class to interact with OpenAI's chat models.\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator # Import classes for creating custom evaluators.\n",
    "\n",
    "model = \"gpt-3.5-turbo\" # Specify the OpenAI model we want to use for our test.\n",
    "\n",
    "\n",
    "# This is your model/system that you want to evaluate.\n",
    "def predict_result(input_: dict) -> dict:\n",
    "    # This function calls the OpenAI model with the provided prompt.\n",
    "    response = ChatOpenAI(model=model).invoke(input_[\"prompt_template\"])\n",
    "    # It then returns the model's output in the standard dictionary format.\n",
    "    return {\"output\": response.content}\n",
    "\n",
    "\n",
    "# The '@run_evaluator' decorator registers this function as a LangSmith evaluator.\n",
    "@run_evaluator\n",
    "def compare_label(run, example) -> EvaluationResult:\n",
    "    # Custom evaluators let you define how \"exact\" the match ought to be.\n",
    "    # 'run' contains information about the model's execution, including its outputs.\n",
    "    # 'example' contains information from the dataset, including the reference output.\n",
    "    \n",
    "    # Flexibly pick the fields to compare by accessing the dictionaries.\n",
    "    prediction = run.outputs.get(\"output\") or \"\" # Get the predicted output string from the run, defaulting to an empty string if not found.\n",
    "    target = example.outputs.get(\"output\") or \"\" # Get the target (reference) output string from the example.\n",
    "    \n",
    "    # Perform the direct string comparison.\n",
    "    match = prediction and prediction == target\n",
    "    \n",
    "    # Return the result in the required EvaluationResult format.\n",
    "    return EvaluationResult(key=\"matches_label\", score=match)\n",
    "\n",
    "\n",
    "# This defines how you generate metrics about the model's performance.\n",
    "eval_config = RunEvalConfig(\n",
    "    # Specify a list of built-in evaluators. `\"exact_match\"` performs the same logic as our custom one.\n",
    "    evaluators=[\"exact_match\"], \n",
    "    # Specify a list of custom evaluator functions to run.\n",
    "    custom_evaluators=[compare_label],\n",
    ")\n",
    "\n",
    "# This is the main function that executes the evaluation.\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name, # The name of the dataset in LangSmith to use for evaluation.\n",
    "    llm_or_chain_factory=predict_result, # A reference to the function/chain that will be tested.\n",
    "    evaluation=eval_config, # The evaluation configuration object we defined above.\n",
    "    verbose=True, # Prints progress and links to the results in LangSmith.\n",
    "    # Add any metadata to the project to help with tracking and organization.\n",
    "    project_metadata={\"version\": \"1.0.0\", \"model\": model},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906501a8",
   "metadata": {},
   "source": [
    "### Role in a Broader Evaluation Framework\n",
    "In this project, exact match serves as a reference point rather than a target metric. Later evaluations introduce semantic judges, trajectory-level analysis, and simulation-based methods that relax the strict assumptions made here. Comparing those methods against exact match helps clarify what each evaluation technique is sensitive to, and where they diverge.\n",
    "\n",
    "By grounding the evaluation suite with a simple, transparent baseline, we can better interpret the behaviour of more complex evaluators and avoid attributing meaning to improvements that are purely artefacts of metric choice.\n",
    "\n",
    "## Discussion\n",
    "While exact match alone is inadequate for evaluating agentic or generative systems, its inclusion is intentional. It provides a clear illustration of how different evaluation choices surface different classes of failure and why relying on a single metric can be misleading when assessing system reliability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
