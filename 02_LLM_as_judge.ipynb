{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33c67",
   "metadata": {},
   "source": [
    "## LLM-as-Judge for Semantic Evaluation\n",
    "\n",
    "### Motivation\n",
    "Exact-match metrics are often too restrictive for evaluating generative systems, especially when multiple responses can be semantically correct. To address this, recent work has explored using large language models themselves as evaluators, assigning scores based on semantic correctness rather than string-level overlap. This notebook examines LLM-as-judge evaluation as a practical but imperfect tool for assessing open-ended model outputs.\n",
    "\n",
    "The goal here is not to treat LLM judgments as ground truth, but to study when and how they provide useful signal, and where they may introduce new sources of error or bias.\n",
    "\n",
    "> **Note 1:** This walkthrough focuses on testing the end-to-end performance of the system. It's also crucial to evaluate individual components. For instance, the retriever can be tested separately using standard information retrieval metrics (e.g., hit rate, MRR) to ensure it's fetching relevant documents effectively.\n",
    "\n",
    "> **Note 2:** If your knowledge base (the documents your system answers from) is constantly changing, your reference answers might become outdated. It's important to have a strategy to manage this, such as freezing the knowledge source during testing or regularly updating your evaluation dataset.\n",
    "\n",
    "### Experimental Setup\n",
    "We use a high-capability language model as a grader, prompting it to compare a system’s output against a reference answer and assign a correctness score. The evaluation is applied across a fixed dataset, allowing direct comparison with simpler baselines such as exact match.\n",
    "\n",
    "The grading prompt is held constant across runs to reduce variability, but the evaluator model remains stochastic, reflecting a realistic deployment scenario where judgments are approximate rather than deterministic.\n",
    "\n",
    "### What LLM-as-Judge Captures\n",
    "LLM-based evaluators are able to detect:\n",
    "- semantic equivalence despite surface-level variation,\n",
    "- partially correct answers that capture core intent,\n",
    "- answers that are fluent but factually inconsistent with the reference.\n",
    "\n",
    "In this sense, LLM-as-judge aligns more closely with human intuition than strict lexical metrics and can substantially reduce false negatives introduced by exact match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-0e1f2a3b4c5d",
   "metadata": {},
   "source": [
    "### Prerequisites and Setup\n",
    "\n",
    "- **`LANGCHAIN_ENDPOINT`**: This URL tells LangChain to send all tracing data to the LangSmith platform.\n",
    "- **`LANGCHAIN_API_KEY`**: This is your secret key for authenticating with LangSmith.\n",
    "- **`PROJECT_NAME`**: (Optional) This allows you to group related runs in LangSmith under a specific project. It's highly recommended for organization.\n",
    "\n",
    "This tutorial uses OpenAI models, ChromaDB for the vector store, and LangChain for building the RAG chain. You will also need to set your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c788783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system's environment variables.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the API endpoint for LangSmith.\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your personal LangSmith API key.\n",
    "project_name = \"YOUR PROJECT NAME\"  # Update with a name for your LangSmith project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe780fbd",
   "metadata": {},
   "source": [
    "- `langchain[openai]`: Installs the core LangChain library along with the specific integrations for OpenAI models.\n",
    "- `chromadb`: The vector database we will use to store and retrieve document embeddings.\n",
    "- `lxml`: A robust parser for HTML and XML, used by our document loader.\n",
    "- `html2text`: A utility to convert HTML into clean, readable plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9e7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%pip install' command installs python packages. The '> /dev/null' part suppresses the output for a cleaner notebook.\n",
    "# %pip install -U \"langchain[openai]\" > /dev/null\n",
    "# %pip install chromadb > /dev/null\n",
    "# %pip install lxml > /dev/null\n",
    "# %pip install html2text > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c4d5-e6f7-4a8b-9c0d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "Set your OpenAI API Key. This is required to use OpenAI's embedding and language models. Replace `<YOUR-API-KEY>` with your actual key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afac8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%env' magic command sets an environment variable for the notebook session.\n",
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ab6e",
   "metadata": {},
   "source": [
    "## Create a Dataset\n",
    "\n",
    "For our Q&A system, the dataset will consist of question-answer pairs. The questions represent typical user queries, and the answers are the \"ground truth\" or reference responses we expect the system to provide.\n",
    "\n",
    "For this example, we'll create a dataset about LangSmith documentation. We have hard-coded a few examples below. For a real-world scenario, it's best to have a much larger dataset (e.g., >100 examples) to get statistically significant results. These examples should ideally be sourced from real user interactions to ensure they are representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e98a80-bf37-457e-b31d-952292e76c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a list of tuples, where each tuple contains a (question, answer) pair.\n",
    "examples = [\n",
    "    (\n",
    "        \"What is LangChain?\", # This is the input question.\n",
    "        \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\", # This is the reference answer.\n",
    "    ),\n",
    "    (\n",
    "        \"How might I query for all runs in a project?\",\n",
    "        \"client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})\",\n",
    "    ),\n",
    "    (\n",
    "        \"What's a langsmith dataset?\",\n",
    "        \"A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How do I use a traceable decorator?\",\n",
    "        \"\"\"The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,\\\n",
    "import the required function, decorate your function, and then call the function. Below is an example:\n",
    "```python\n",
    "from langsmith.run_helpers import traceable\n",
    "@traceable(run_type=\"chain\") # or \"llm\", etc.\n",
    "def my_function(input_param):\n",
    "    # Function logic goes here\n",
    "    return output\n",
    "result = my_function(input_param)\n",
    "```\"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"Can I trace my Llama V2 llm?\",\n",
    "        \"So long as you are using one of LangChain's LLM implementations, all your calls can be traced\",\n",
    "    ),\n",
    "    (\n",
    "        \"Why do I have to set environment variables?\",\n",
    "        \"Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith.\"\n",
    "        \" While there are other ways to connect, environment variables tend to be the simplest way to configure your application.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How do I move my project between organizations?\",\n",
    "        \"LangSmith doesn't directly support moving projects between organizations.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6-e7f8-4a9b-ac0d-2e3f4a5b6c7d",
   "metadata": {},
   "source": [
    "Now, let's create a LangSmith client, which is our main entry point for interacting with the LangSmith platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5edb7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client # Import the Client class from the langsmith library.\n",
    "\n",
    "client = Client() # Instantiate the client. It will automatically use the environment variables we set earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7-e8f9-4a0b-ad1d-3e4f5a6b7c8d",
   "metadata": {},
   "source": [
    "Using the client, we will programmatically create a new dataset in LangSmith and populate it with our examples. We add a unique identifier (`uuid`) to the dataset name to prevent naming conflicts if we run this notebook multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbcd3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid # Import the uuid library to generate unique identifiers.\n",
    "\n",
    "# Define a unique name for the dataset using a UUID to avoid collisions.\n",
    "dataset_name = f\"Retrieval QA Questions {str(uuid.uuid4())}\"\n",
    "# Create the dataset on the LangSmith platform and get back a dataset object.\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "# Loop through our list of hard-coded question-answer pairs.\n",
    "for q, a in examples:\n",
    "    # For each pair, create an example in our LangSmith dataset.\n",
    "    client.create_example(\n",
    "        inputs={\"question\": q}, # The input dictionary must have keys that match what our chain expects.\n",
    "        outputs={\"answer\": a}, # The output dictionary contains the ground truth reference answer.\n",
    "        dataset_id=dataset.id # We specify which dataset to add this example to.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976437f",
   "metadata": {},
   "source": [
    "### Define the RAG Q&A System\n",
    "\n",
    "We are using a **Retrieval-Augmented Generation (RAG)** architecture:\n",
    "\n",
    "1.  **Retrieval**: Given a user's question, the system first retrieves relevant information from a knowledge base. In our case, this knowledge base is the LangSmith documentation. This stage consists of:\n",
    "    -   An **Embedding Model** (`OpenAIEmbeddings`): Converts both the documents and the user's question into numerical vectors (embeddings).\n",
    "    -   A **Vector Store** (`Chroma`): A specialized database that stores the document vectors and allows for efficient searching to find vectors (and thus documents) that are most similar to the question vector.\n",
    "    -   A **Retriever**: The component that orchestrates the search in the vector store and returns the most relevant documents.\n",
    "\n",
    "2.  **Generation**: The retrieved documents are then passed to an LLM, along with the original question, to generate a final, synthesized answer. This stage consists of:\n",
    "    -   A **Prompt Template** (`ChatPromptTemplate`): Structures the input for the LLM, combining the retrieved context and the user's question with instructions on how to answer.\n",
    "    -   An **LLM** (`ChatOpenAI`): The language model that reads the prompt and generates the textual response.\n",
    "\n",
    "We will use LangChain Expression Language (LCEL) to elegantly combine these components into a single, executable chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95fab721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mukilloganathan/langchain/venv/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader # A loader for recursively scraping a website.\n",
    "from langchain_community.document_transformers import Html2TextTransformer # A transformer to convert HTML content to plain text.\n",
    "from langchain_community.vectorstores import Chroma # The Chroma vector store implementation.\n",
    "from langchain_text_splitters import TokenTextSplitter # A text splitter that splits based on token count.\n",
    "from langchain_openai import OpenAIEmbeddings # The class for using OpenAI's embedding models.\n",
    "\n",
    "# Initialize a loader to fetch all documents from the LangSmith documentation website.\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\")\n",
    "# Initialize a text splitter to break large documents into smaller chunks.\n",
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\", # The model used to count tokens for splitting.\n",
    "    chunk_size=2000, # The maximum size of each chunk in tokens.\n",
    "    chunk_overlap=200, # The number of tokens to overlap between consecutive chunks.\n",
    ")\n",
    "# Initialize a transformer to clean up the raw HTML.\n",
    "doc_transformer = Html2TextTransformer()\n",
    "# Load the raw documents from the URL.\n",
    "raw_documents = api_loader.load()\n",
    "# Transform the raw HTML documents into plain text.\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "# Split the transformed documents into smaller, manageable chunks.\n",
    "documents = text_splitter.split_documents(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc4b79-4219-4446-a00c-beda55c2205a",
   "metadata": {},
   "source": [
    "With the documents processed, we can now create the vector store and the retriever. The vector store will embed and index our document chunks, and the retriever will provide the interface for searching them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "057f0841-dd9f-4f75-8ff5-dbdda73f84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI embeddings model.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# Create a Chroma vector store from the documents, using the OpenAI embeddings model.\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "# Create a retriever from the vector store, configured to return the top 4 most relevant documents.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f266e9-e4de-42ad-b41e-99ace4dc5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime # Import datetime to include the current time in the prompt.\n",
    "from langchain_core.output_parsers import StrOutputParser # Import the string output parser.\n",
    "from langchain_core.prompts import ChatPromptTemplate # Import the chat prompt template class.\n",
    "from langchain_openai import ChatOpenAI # Import the OpenAI chat model class.\n",
    "\n",
    "# Define the prompt template. This structures the input for the LLM.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", # The system message provides high-level instructions to the model.\n",
    "            \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "            \" questions from LangSmith's documentation.\"\n",
    "            \" LangChain is a framework for building applications using large language models.\"\n",
    "            \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"), # A placeholder for the retrieved documents (context).\n",
    "        (\"human\", \"{question}\"), # A placeholder for the user's question.\n",
    "    ]\n",
    ").partial(time=str(datetime.now())) # Pre-fill the 'time' variable with the current time.\n",
    "\n",
    "# Initialize the LLM we'll use for generation. We use gpt-3.5-turbo with a large context window and low temperature for factual answers.\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "# Define the response generator part of the chain using LCEL. It pipes the prompt to the model, then to an output parser.\n",
    "response_generator = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4490e622-d865-44ee-b6f4-681b658dad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full chain combines the retriever and the response generator.\n",
    "from operator import itemgetter # Import itemgetter for convenient data routing.\n",
    "\n",
    "chain = (\n",
    "    # A Runnable Map takes the input dictionary and prepares a new dictionary for the next step.\n",
    "    {\n",
    "        # The 'context' key is populated by a sub-chain: get the question, pass it to the retriever, and format the resulting documents.\n",
    "        \"context\": itemgetter(\"question\")\n",
    "        | retriever\n",
    "        | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "        # The 'question' key is passed through directly from the input.\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | response_generator # The output of the map is piped into our response generator chain.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77b0954d-924b-4241-9c59-96adbd1c3ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To log user feedback to a run in LangSmith, you can use the `create_feedback` method provided by the LangSmith client. Here's an example of how to log user feedback using the Python client:\n",
      "\n",
      "```python\n",
      "from langsmith import Client\n",
      "\n",
      "client = Client()\n",
      "\n",
      "# Specify the run ID and feedback key\n",
      "run_id = \"<run_id>\"\n",
      "feedback_key = \"thumbs_up\"\n",
      "\n",
      "# Log the feedback\n",
      "client.create_feedback(\n",
      "    run_id,\n",
      "    feedback_key,\n",
      "    score=True\n",
      ")\n",
      "```\n",
      "\n",
      "In this example, we log a \"thumbs up\" feedback for a specific run by calling `create_feedback` with the run ID, feedback key, and a score of `True`. You can customize the feedback by providing additional optional fields such as `value`, `correction`, `comment`, `source_info`, and `feedback_source_type`.\n",
      "\n",
      "You can also log feedback using the LangSmith client in TypeScript. Here's an example:\n",
      "\n",
      "```typescript\n",
      "import { Client } from \"langsmith\";\n",
      "\n",
      "const client = new Client();\n",
      "\n",
      "// Specify the run ID and feedback key\n",
      "const runId = \"<run_id>\";\n",
      "const feedbackKey = \"thumbs_up\";\n",
      "\n",
      "// Log the feedback\n",
      "await client.createFeedback(runId, feedbackKey, { score: true });\n",
      "```\n",
      "\n",
      "Remember to replace `<run_id>` with the actual ID of the run you want to log feedback for, and `<feedback_key>` with the desired feedback key."
     ]
    }
   ],
   "source": [
    "# We stream the output of the chain for a sample question.\n",
    "for tok in chain.stream({\"question\": \"How do I log user feedback to a run?\"}):\n",
    "    print(tok, end=\"\", flush=True) # Print each token as it is generated for a real-time effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dddd61",
   "metadata": {},
   "source": [
    "### Evaluate the Chain\n",
    "\n",
    "We will use one of LangSmith's built-in, LLM-assisted evaluators called `\"qa\"`. This evaluator is specifically designed for Q&A tasks. For each example in our dataset, it will:\n",
    "1.  Receive the generated answer from our RAG chain.\n",
    "2.  Receive the reference answer from our dataset.\n",
    "3.  Use an LLM to determine if the generated answer is a \"correct\" answer based on the reference. It returns a binary score (1 for correct, 0 for incorrect).\n",
    "\n",
    "We configure this using the `RunEvalConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dedaff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig # Import the evaluation configuration class.\n",
    "\n",
    "# Create an evaluation configuration object.\n",
    "eval_config = RunEvalConfig(\n",
    "    # Specify the evaluators to use. 'qa' is a built-in evaluator for question-answering correctness.\n",
    "    evaluators=[\"qa\"],\n",
    "    # You can optionally configure the LLM used for evaluation if you want to use a different model.\n",
    "    # eval_llm=ChatAnthropic(model=\"claude-2\", temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5e4f7-f5b4-4d6a-9dd7-1973d8e9c0f7",
   "metadata": {},
   "source": [
    "Now we execute the evaluation. The `client.arun_on_dataset` function orchestrates the entire process. It iterates through each example in our dataset, runs our RAG chain on the input question, and then applies the `qa` evaluator to score the result. The `await` keyword is used because this is an asynchronous operation, which can run evaluations in parallel for greater efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30ce874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-virtual-attitude-2' at:\n",
      "https://smith.langchain.com/o/9a6371ef-ea6a-4860-b3bd-9614084873e7/projects/p/b539e1db-d7db-4da7-87c3-d9087ed5d0b9\n",
      "[------------------------------------------------->] 7/7"
     ]
    }
   ],
   "source": [
    "# Asynchronously run the evaluation on the dataset.\n",
    "_ = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name, # The name of the dataset to test against.\n",
    "    llm_or_chain_factory=lambda: chain, # A function that returns an instance of the chain to be tested.\n",
    "    evaluation=eval_config, # The evaluation configuration we defined earlier.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fac17c-e0c7-4d85-bca1-18c4337fbcba",
   "metadata": {},
   "source": [
    "### Analyzing the Results in LangSmith\n",
    "\n",
    "As the test progresses, you can click the link printed above to go to the LangSmith project. There, you can see real-time results, including the chain's outputs, the feedback scores from the evaluator, and detailed traces for each run.\n",
    "\n",
    "To find problematic examples, you can filter the results. For example, to see all the runs that the `qa` evaluator marked as incorrect, you can filter for `\"Correctness==0\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15270951-84df-4ac8-84ef-23312fc16db0",
   "metadata": {},
   "source": [
    "### Diagnosing and Fixing the Error\n",
    "In this example, one of the traces was marked as \"incorrect\". By inspecting the trace, we might find that the model is \"hallucinating\".\n",
    "\n",
    "To fix the hallucination, we can try making the prompt more robust. \n",
    "\n",
    "> Respond as best as you can. If no documents are retrieved or if you do not see an answer in the retrieved documents, admit you do not know or that you don't see it being supported at the moment.\n",
    "\n",
    "After adding this message in the Playground and resubmitting, we can see if the model's behavior improves.\n",
    "\n",
    "The new prompt seems to fix the issue for this specific example. However, we need to ensure this change doesn't negatively affect other examples (i.e., we're not overfitting to a single failure case). The next step is to re-run the entire evaluation with our improved chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbd6a6-e3eb-4d3a-afe6-d5917d81c54d",
   "metadata": {},
   "source": [
    "### Iterate and Re-Evaluate\n",
    "\n",
    "Evaluation is not a one-time event; it's a cycle. Below, we define a new RAG chain (`chain_2`) that includes the improved prompt with the added system message to discourage hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d189e9-ff07-48d9-9aef-7fc17b265e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new, improved prompt template.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "            \" questions from LangSmith's documentation.\"\n",
    "            \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "        # Add the new system message here to make the model more cautious:\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Respond as best as you can. If no documents are retrieved or if you do not see an answer in the retrieved documents,\"\n",
    "            \" admit you do not know or that you don't see it being supported at the moment.\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(time=lambda: str(datetime.now())) # Use a lambda to get the current time dynamically for each run.\n",
    "\n",
    "# Re-initialize the model and response generator with the new prompt.\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "response_generator_2 = prompt | model | StrOutputParser()\n",
    "# Assemble the second version of our RAG chain.\n",
    "chain_2 = {\n",
    "    \"context\": itemgetter(\"question\")\n",
    "    | retriever\n",
    "    | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "} | response_generator_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbdfa2f4-fa96-42d9-94bc-bb5c227104da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-impressionable-lead-60' at:\n",
      "https://smith.langchain.com/o/9a6371ef-ea6a-4860-b3bd-9614084873e7/projects/p/fc4f3319-1707-4035-b4e8-b3b3fafcf5b7\n",
      "[------------------------------------------------->] 7/7"
     ]
    }
   ],
   "source": [
    "# Run the evaluation again with the updated chain factory.\n",
    "_ = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name, # Use the same dataset as before.\n",
    "    llm_or_chain_factory=lambda: chain_2, # Point to the new, improved chain.\n",
    "    evaluation=eval_config, # Use the same evaluation configuration.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5352e20-af87-4735-9061-caf09e86610c",
   "metadata": {},
   "source": [
    "### Limitations and Failure Modes\n",
    "LLM-based evaluation introduces its own risks. The judge may:\n",
    "- reward plausible but incorrect reasoning,\n",
    "- overvalue fluency or verbosity,\n",
    "- reflect its own biases or blind spots,\n",
    "- vary across runs in subtle but consequential ways.\n",
    "\n",
    "Importantly, the evaluator has no privileged access to the true causal process that generated the answer. As a result, agreement between a model and an LLM judge should not be conflated with faithfulness or correctness in a stronger sense.\n",
    "\n",
    "### Role in a Broader Evaluation Framework\n",
    "In this project, LLM-as-judge is treated as one signal among many rather than a definitive arbiter. Its outputs are most informative when compared against simpler baselines and more structured evaluations, such as trajectory analysis or component-wise testing.\n",
    "\n",
    "By examining where LLM-as-judge agrees or disagrees with other metrics, we can better understand which types of errors each method surfaces—and which remain hidden.\n",
    "\n",
    "## Discussion\n",
    "LLM-as-judge offers a scalable and flexible way to evaluate generative systems, but it shifts the problem rather than solving it: evaluation becomes another modelling task with its own assumptions and failure modes. This notebook treats LLM-based evaluation as an object of study in its own right, highlighting both its utility and its limits within a safety-oriented evaluation pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
