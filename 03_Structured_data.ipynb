{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66854c19-6b01-4046-8ac5-27940537c35c",
   "metadata": {},
   "source": [
    "## Evaluating Structured Outputs in LLM Pipelines\n",
    "\n",
    "### Motivation\n",
    "Many real-world LLM applications rely on structured outputs, such as JSON, schemas, or key–value records, that are consumed directly by downstream systems. In these settings, failures are often silent: an output may appear fluent and plausible while violating structural constraints in ways that break execution or corrupt data. This notebook treats structured-output evaluation as a way to study how reliably models adhere to explicit specifications under generative uncertainty.\n",
    "\n",
    "Rather than focusing on natural language correctness, the emphasis here is on whether models can consistently map unstructured inputs to well-formed, machine-readable representations.\n",
    "\n",
    "###Experimental Setup\n",
    "We evaluate model outputs that are intended to follow a predefined schema. Instead of binary validity checks alone, we use distance-based metrics (e.g. edit distance over JSON structures) to quantify how an output deviates from the target structure when it fails.\n",
    "\n",
    "This allows us to distinguish between superficial formatting errors and deeper semantic or structural mismatches, which often have very different implications in production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### Prerequisites and Setup\n",
    "\n",
    "- `langchain`, `langsmith`, `langchain_experimental`: The core libraries for building the chain, connecting to LangSmith for evaluation, and accessing experimental features like Anthropic Functions.\n",
    "- `anthropic`: The Python SDK for the Anthropic (Claude) API.\n",
    "- `jsonschema`: A dependency used by LangChain's extraction tools to validate the structure of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464edf8-751b-4b80-9f01-70af40f00c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%pip install' command installs python packages from the notebook.\n",
    "# -U ensures we get the latest versions.\n",
    "# --quiet suppresses the installation output for a cleaner interface.\n",
    "%pip install -U --quiet langchain langsmith langchain_experimental anthropic jsonschema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-b8c9-d0e1-f2a3b4c5d6e7",
   "metadata": {},
   "source": [
    "- **`LANGCHAIN_API_KEY`**: Your secret key for authenticating with LangSmith, which enables logging and evaluation.\n",
    "- **`ANTHROPIC_API_KEY`**: Your secret key for the Anthropic API, required to use Claude models.\n",
    "- **`LANGCHAIN_ENDPOINT`**: This URL directs all LangChain tracing data to the LangSmith platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3b17b-736a-4991-8122-11bf3ac121c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system's environment variables.\n",
    "import uuid # Import the 'uuid' module, though it is not used in this specific cell, it's good practice for creating unique IDs.\n",
    "\n",
    "uid = uuid.uuid4() # Create a unique identifier object (not used here but declared).\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\" # Set your LangSmith API key as an environment variable.\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-***\" # Set your Anthropic (Claude) API key as an environment variable.\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876b92c-74a5-47ae-9efc-dab64bf88d19",
   "metadata": {},
   "source": [
    "### Create the Evaluation Dataset\n",
    "\n",
    "For this task, we need examples that pair unstructured text (a contract) with its corresponding structured representation (the filled-out contract details).\n",
    "\n",
    "We will use a pre-existing public dataset in LangSmith, which is derived from the Contract Understanding Atticus Dataset (CUAD). LangSmith's ability to share and clone datasets is incredibly useful for collaboration and reproducibility. The `clone_public_dataset` function will copy this public dataset into your own LangSmith account, allowing you to run evaluations against it.\n",
    "\n",
    "You can explore the original public dataset here: [Contract Extraction Dataset](https://smith.langchain.com/public/08ab7912-006e-4c00-a973-0f833e74907b/d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921efddb-8210-4d5f-8705-41e6f9521b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client # Import the Client class to interact with the LangSmith API.\n",
    "\n",
    "# The URL of the public dataset we want to use.\n",
    "dataset_url = (\n",
    "    \"https://smith.langchain.com/public/08ab7912-006e-4c00-a973-0f833e74907b/d\"\n",
    ")\n",
    "# Define a name for our local copy of the dataset.\n",
    "dataset_name = f\"Contract Extraction\"\n",
    "\n",
    "# Instantiate the LangSmith client.\n",
    "client = Client()\n",
    "# Use the client to clone the public dataset into your LangSmith account.\n",
    "client.clone_public_dataset(dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a9e51-7fcb-4de5-87af-644b7ca9b893",
   "metadata": {},
   "source": [
    "### Define the Extraction Chain\n",
    "\n",
    "The first step in defining our chain is to create the target schema using Pydantic. This tells the LLM exactly what information to look for and how to structure it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51793dc-ee9f-491c-aa91-fb32cf66308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union # Import typing hints for defining our data models.\n",
    "\n",
    "from pydantic import BaseModel # Import BaseModel from Pydantic to create our data schemas.\n",
    "\n",
    "\n",
    "# Define the schema for a physical address.\n",
    "class Address(BaseModel):\n",
    "    street: str # The street address.\n",
    "    city: str # The city.\n",
    "    state: str # The state or province.\n",
    "    zip_code: str # The postal or zip code.\n",
    "    country: Optional[str] # The country, marked as optional.\n",
    "\n",
    "\n",
    "# Define the schema for a party involved in the contract.\n",
    "class Party(BaseModel):\n",
    "    name: str # The name of the party.\n",
    "    address: Address # A nested Address object.\n",
    "    type: Optional[str] # The type of party (e.g., \"Landlord\", \"Tenant\"), marked as optional.\n",
    "\n",
    "\n",
    "# Define the schema for a single section of the contract.\n",
    "class Section(BaseModel):\n",
    "    title: str # The title of the section.\n",
    "    content: str # The full text content of the section.\n",
    "\n",
    "\n",
    "# Define the top-level schema for the entire contract.\n",
    "class Contract(BaseModel):\n",
    "    document_title: str # The main title of the contract document.\n",
    "    exhibit_number: Optional[str] # Any exhibit number associated with the contract, optional.\n",
    "    effective_date: str # The date the contract becomes effective.\n",
    "    parties: List[Party] # A list of Party objects.\n",
    "    sections: List[Section] # A list of Section objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e90fd2-bd7c-499d-b02d-3eee98550031",
   "metadata": {},
   "source": [
    "With our Pydantic schema defined, we can now construct the full extraction chain. This chain will take the raw text of a contract as input and produce the structured `Contract` object as output. We will use LangChain Expression Language (LCEL) to pipe the components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a18eb63-c525-40d6-be1d-bd4c52fbfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub # Import the hub to pull pre-made prompts.\n",
    "from langchain.chains import create_extraction_chain # Import a helper function to easily create an extraction chain.\n",
    "from langchain_anthropic import ChatAnthropic # Import the Anthropic chat model wrapper.\n",
    "from langchain_experimental.llms.anthropic_functions import AnthropicFunctions # Import the experimental Anthropic Functions wrapper.\n",
    "\n",
    "# Pull a prompt that is specifically designed for contract extraction with Anthropic models.\n",
    "contract_prompt = hub.pull(\"wfh/anthropic_contract_extraction\")\n",
    "\n",
    "\n",
    "# Create the core extraction logic as a sub-chain.\n",
    "extraction_subchain = create_extraction_chain(\n",
    "    Contract.schema(), # Provide the Pydantic schema that we want to extract.\n",
    "    llm=AnthropicFunctions(model=\"claude-2.1\", max_tokens=20_000), # Use the Anthropic Functions LLM, specifying a model and max tokens.\n",
    "    prompt=contract_prompt, # Provide the specialized prompt from the hub.\n",
    ")\n",
    "# The dataset provides input with a key named \"context\", but our extraction_subchain expects a key named \"input\".\n",
    "# We use LCEL to create a final chain that correctly maps the keys.\n",
    "chain = (\n",
    "    # This lambda function takes the original input 'x' and transforms it into the format the subchain expects.\n",
    "    (lambda x: {\"input\": x[\"context\"]})\n",
    "    | extraction_subchain # The transformed input is piped into our extraction subchain.\n",
    "    # The subchain's output is `{'text': [...]}`. We transform it again to `{'output': [...]}` for the evaluator.\n",
    "    | (lambda x: {\"output\": x[\"text\"]})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee760e32-6d0b-421a-b73b-c15c60f35883",
   "metadata": {},
   "source": [
    "### Evaluate the Chain\n",
    "\n",
    "For structured data like JSON, a simple string comparison is often too strict. The order of keys in a JSON object can change without altering the meaning of the data. \n",
    "\n",
    "To handle this, we'll use the **`json_edit_distance`** evaluator. This is a powerful, non-LLM-based evaluator that works as follows:\n",
    "1.  **Canonicalization**: It takes both the predicted JSON and the reference JSON and standardizes them. This typically involves sorting all keys alphabetically at every level of the object.\n",
    "2.  **String Conversion**: It converts the canonical JSON objects into strings.\n",
    "3.  **Edit Distance**: It calculates the Damerau-Levenshtein edit distance between the two strings. This measures the number of insertions, deletions, substitutions, and transpositions required to change one string into the other.\n",
    "4.  **Normalization**: The raw distance is normalized to produce a similarity score between 0.0 (completely different) and 1.0 (identical)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4-a5b6-c7d8-e9f0-a1b2c3d4e5f6",
   "metadata": {},
   "source": [
    "Before running the evaluation, we'll adjust the logging level. The legal documents are very long, and if an error occurs during processing, the default logging behavior might print the entire document to the screen, cluttering the notebook. By setting the logging level to `CRITICAL`, we ensure that only the most severe errors are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d8ff4f-f5fa-4f21-a412-9a8b21749ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging # Import the logging module to control log message verbosity.\n",
    "\n",
    "# We will suppress any non-critical errors here since the documents are long\n",
    "# and could pollute the notebook output with excessive text.\n",
    "logger = logging.getLogger() # Get the root logger.\n",
    "logger.setLevel(logging.CRITICAL) # Set its level to CRITICAL, so only messages of that severity or higher will be shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3f4a5-b6c7-d8e9-f0a1-b2c3d4e5f6a7",
   "metadata": {},
   "source": [
    "Finally, we run the evaluation using LangSmith's `evaluate` function. This function orchestrates the entire process: it takes our chain, runs it on each example from our dataset, and then applies the specified evaluator to score the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03383cdd-8fef-480f-bddf-b0616ed6e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/6rnp_mbx5914kx7qmmh7xzmw0000gn/T/ipykernel_72212/1408924987.py:3: UserWarning: Function evaluate is in beta.\n",
      "  res = evaluate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ordinary-hat-82' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/fbc1a41c-7043-4b5f-b6e8-78266faac187/compare?selectedSessions=02fbe581-47ae-4c87-bcad-7a8c44e8789b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65138131988b4c1fa75e2d21cf74fb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate # Import the necessary evaluation functions from LangSmith.\n",
    "\n",
    "# Call the main evaluate function to run the experiment.\n",
    "res = evaluate(\n",
    "    chain.invoke, # The function to be tested. `chain.invoke` is the standard way to run a chain.\n",
    "    data=dataset_name, # The name of the dataset in LangSmith to run on.\n",
    "    evaluators=[LangChainStringEvaluator(\"json_edit_distance\")], # A list of evaluators to apply to the results.\n",
    "    # To avoid hitting API rate limits, we can limit the number of concurrent requests.\n",
    "    max_concurrency=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851fdbd-64c1-4399-8b2a-4ea04e16fe56",
   "metadata": {},
   "source": [
    "### Analyze the Results\n",
    "\n",
    "- Look at examples with low scores. What kind of errors is the model making? Is it missing entire sections, or just making small mistakes in fields like dates or addresses?\n",
    "- Are there any examples where the model seems to hallucinate information that wasn't in the original text?\n",
    "- Could the reference data in the dataset be improved? Sometimes, evaluation failures can point to ambiguities or errors in the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59fb9d-9cb1-4365-9619-28d2897e0dfd",
   "metadata": {},
   "source": [
    "### What Structured Validation Reveals\n",
    "Structured evaluation is effective at detecting:\n",
    "- partial compliance with a schema,\n",
    "- hallucinated or missing fields,\n",
    "- incorrect nesting or type violations,\n",
    "- outputs that are semantically plausible but operationally invalid.\n",
    "\n",
    "These failure modes are frequently missed by semantic evaluators, which may reward fluency while overlooking violations that would cause downstream systems to fail.\n",
    "\n",
    "### Limitations and Trade-offs\n",
    "While structured validation provides clear signals about specification adherence, it does not assess whether the content of the output is correct or meaningful. A perfectly valid schema can still encode incorrect information. As such, structured evaluation should not be interpreted as a proxy for correctness, but as a complementary diagnostic focused on interface reliability.\n",
    "\n",
    "### Role in a Broader Evaluation Framework\n",
    "In this project, structured-output evaluation functions as a boundary check between generative models and deterministic systems. When combined with semantic evaluation and trajectory-level analysis, it helps localise where failures occur: whether a model misunderstood the task, violated constraints during generation, or produced a structurally valid but incorrect result.\n",
    "\n",
    "This distinction is critical for debugging and for deciding where to intervene—prompting, decoding, post-processing, or system design.\n",
    "\n",
    "## Discussion\n",
    "As LLMs are increasingly embedded in pipelines that expect precise, machine-readable outputs, structural reliability becomes a first-order concern. This notebook demonstrates how structured validation can be used not only to catch errors, but to characterise the kinds of failures models make when translating between unstructured reasoning and formal representations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
