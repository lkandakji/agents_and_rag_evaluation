{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce8c14f-0412-4d1b-a3fd-ad195daf3622",
   "metadata": {},
   "source": [
    "## Dynamic Ground Truth for Evaluating Non-Stationary Tasks\n",
    "\n",
    "### Motivation\n",
    "Many evaluation setups implicitly assume that the ground truth is static. This assumption breaks down in settings where answers depend on external state: live databases, changing APIs, time-dependent facts, or evolving environments. In these cases, storing a fixed reference answer can be misleading, as a system may be penalised for producing a response that is correct at execution time but differs from an outdated label.\n",
    "\n",
    "This notebook explores dynamic ground truth evaluation, where correctness is defined by executable logic rather than static annotations.\n",
    "\n",
    "### Experimental Setup\n",
    "Instead of storing answers directly, the dataset encodes ground truth as executable functions. At evaluation time, these functions are run to generate the correct answer based on the current state of the underlying data source.\n",
    "\n",
    "This approach allows evaluation to remain aligned with the task as it actually exists at inference time, rather than with a snapshot of the world taken during dataset creation.\n",
    "\n",
    "### What Dynamic Ground Truth Captures\n",
    "Dynamic evaluation is particularly effective for detecting:\n",
    "- failures caused by stale knowledge,\n",
    "- incorrect tool use when querying live systems,\n",
    "- mismatches between model assumptions and current system state,\n",
    "- fragile reliance on memorised facts instead of retrieval or computation.\n",
    "\n",
    "By recomputing ground truth at runtime, this method separates failures of reasoning from failures of data freshness.\n",
    "\n",
    "> **Note**: We use a simple CSV file and pandas DataFrame to simulate a dynamic data source. This is for illustrative purposes; in a real-world scenario, this could be a SQL database, a GraphQL API, or any other data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### Prerequisites and Setup\n",
    "\n",
    "- **`LANGCHAIN_ENDPOINT`**: This URL tells LangChain to send all tracing data to the LangSmith platform.\n",
    "- **`LANGCHAIN_API_KEY`**: This is your secret key for authenticating with LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b44ce-3c06-4029-b25d-dad3c02808fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system's environment variables.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the API endpoint for LangSmith.\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your personal LangSmith API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbe6e3-062e-41fd-901b-7fe9f2895a0a",
   "metadata": {},
   "source": [
    "- `langchain[openai]`: Installs the core LangChain library and integrations for OpenAI models.\n",
    "- `pandas`: The library for data manipulation and analysis, used here as our data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a536c5d-0b6b-45eb-8dcb-eabdf3caaae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%pip install' command installs python packages. '> /dev/null' suppresses the output.\n",
    "# %pip install -U \"langchain[openai]\" > /dev/null\n",
    "# %pip install pandas > /dev/null\n",
    "# The '%env' magic command sets an environment variable for the notebook session.\n",
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2587dd-19e4-4748-b211-8a97d0c7c5a5",
   "metadata": {},
   "source": [
    "### Create a Dataset with Dynamic References\n",
    "\n",
    "We will use the classic Titanic dataset as our data source. The key difference in our approach is how we define the labels. Instead of calculating the answers beforehand and storing them as static values, we will store Python code snippets that can be executed on the DataFrame to get the correct answer.\n",
    "\n",
    "This is the principle of **indirection** in action. The label is not the answer itself, but a *recipe for finding the answer*. This ensures that our evaluation always compares against the most up-to-date data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d51bd9b-bea5-4670-a3a1-e24d853e2a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a list of tuples, where each tuple is a (question, code_snippet) pair.\n",
    "questions = [\n",
    "    (\"How many passengers were on the Titanic?\", \"len(df)\"),\n",
    "    (\"How many passengers survived?\", \"df['Survived'].sum()\"),\n",
    "    (\"What was the average age of the passengers?\", \"df['Age'].mean()\"),\n",
    "    (\"How many male and female passengers were there?\", \"df['Sex'].value_counts()\"),\n",
    "    (\"What was the average fare paid for the tickets?\", \"df['Fare'].mean()\"),\n",
    "    (\"How many passengers were in each class?\", \"df['Pclass'].value_counts()\"),\n",
    "    (\n",
    "        \"What was the survival rate for each gender?\",\n",
    "        \"df.groupby('Sex')['Survived'].mean()\",\n",
    "    ),\n",
    "    (\n",
    "        \"What was the survival rate for each class?\",\n",
    "        \"df.groupby('Pclass')['Survived'].mean()\",\n",
    "    ),\n",
    "    (\n",
    "        \"Which port had the most passengers embark from?\",\n",
    "        \"df['Embarked'].value_counts().idxmax()\",\n",
    "    ),\n",
    "    (\n",
    "        \"How many children under the age of 18 survived?\",\n",
    "        \"df[df['Age'] < 18]['Survived'].sum()\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd5b2c0-1999-4015-8f43-18f5c3edaff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid # Import the uuid library to generate unique identifiers.\n",
    "\n",
    "from langsmith import Client # Import the Client class to interact with LangSmith.\n",
    "\n",
    "client = Client() # Instantiate the LangSmith client.\n",
    "# Define a unique name for the dataset using a short hex code from a UUID.\n",
    "dataset_name = f\"Dynamic Titanic CSV {uuid.uuid4().hex[:4]}\"\n",
    "# Create the dataset on the LangSmith platform.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, # The name for the new dataset.\n",
    "    description=\"Test QA over CSV\", # An optional description for the dataset.\n",
    ")\n",
    "\n",
    "# Create all the examples in the dataset in a single API call for efficiency.\n",
    "client.create_examples(\n",
    "    # The inputs are a list of dictionaries, each with a 'question' key.\n",
    "    inputs=[{\"question\": example[0]} for example in questions],\n",
    "    # The outputs are a list of dictionaries, each with a 'code' key containing the reference snippet.\n",
    "    outputs=[{\"code\": example[1]} for example in questions],\n",
    "    dataset_id=dataset.id, # Link these examples to the dataset we just created.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acabdc82-d426-4432-a09d-1daa560f08ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the Q&A System\n",
    "\n",
    "With the dataset created, it's time to define our question-answering system. We'll use a pre-built LangChain component: the **pandas dataframe agent**. This agent is specifically designed to answer questions about a pandas DataFrame by generating and executing Python code.\n",
    "\n",
    "First, we load the Titanic data into a DataFrame. Then, we create a constructor function for our agent that we can pass to the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc67277-1191-4409-9f00-67d8e55e2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Import the pandas library for data manipulation.\n",
    "\n",
    "# The URL of the raw CSV file for the Titanic dataset.\n",
    "titanic_path = \"https://raw.githubusercontent.com/jorisvandenbossche/pandas-tutorial/master/data/titanic.csv\"\n",
    "# Read the CSV data from the URL into a pandas DataFrame.\n",
    "df = pd.read_csv(titanic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4-f5a6-b7c8-d9e0-f1a2b3c4d5e6",
   "metadata": {},
   "source": [
    "Now, we define the `predict` function. This function will be our \"system under test\". For each run, it initializes a new pandas dataframe agent with our designated LLM and the current state of the DataFrame `df`. It then invokes the agent with the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8555151b-aa5d-480b-962f-a2790f0b7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate # Import prompt templates.\n",
    "from langchain_experimental.agents import create_pandas_dataframe_agent # Import the agent constructor.\n",
    "from langchain_openai import ChatOpenAI # Import the OpenAI chat model wrapper.\n",
    "\n",
    "# Initialize the LLM. We use a powerful model like GPT-4 for code generation tasks and set temperature to 0 for deterministic outputs.\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0.0)\n",
    "\n",
    "\n",
    "# Define the function to be evaluated.\n",
    "def predict(inputs: dict):\n",
    "    # Inside the function, create an instance of the pandas dataframe agent.\n",
    "    agent = create_pandas_dataframe_agent(agent_type=\"openai-tools\", llm=llm, df=df)\n",
    "    # Invoke the agent with the question from the input dictionary.\n",
    "    return agent.invoke({\"input\": inputs[\"question\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be5a59c2-7eac-441e-8dd5-a73562ac004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How many passengers were on the Titanic?',\n",
       " 'output': 'There were 891 passengers on the Titanic according to the dataframe.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run an example prediction to see the agent in action.\n",
    "predict({\"question\": \"How many passengers were on the Titanic?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e127f9-ff9c-4ab0-9860-6d6432c02651",
   "metadata": {},
   "source": [
    "### Run Evaluation with a Custom Evaluator\n",
    "\n",
    "We need an evaluator that understands our dynamic labels. We'll create a custom evaluator by inheriting from `LabeledCriteriaEvalChain`. This base class is an LLM-powered evaluator that assesses a prediction based on a given criterion (e.g., \"correctness\") and a reference label.\n",
    "\n",
    "Our customization is simple but powerful: we will override the `_get_eval_input` method. This method is responsible for preparing the inputs that get passed to the evaluator's LLM. In our overridden version, we will first call the parent method to get the standard inputs, and then we will **execute the `reference` value (our code snippet) using Python's `eval()` function**. This replaces the code snippet with its live result.\n",
    "\n",
    "The result is that the evaluator's LLM never sees the code; it only sees the prediction and the freshly fetched, up-to-the-minute correct answer.\n",
    "\n",
    "> **Security Warning**: Using `eval()` on untrusted code is extremely dangerous as it can execute arbitrary commands. In this tutorial, we are only evaluating code that we have written ourselves in a controlled environment. **Never** use this `eval()` approach in a production system where the code snippets could come from untrusted users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b8b84e5-f977-4893-bda7-20a1248469e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional # Import typing hints.\n",
    "\n",
    "from langchain.evaluation.criteria.eval_chain import LabeledCriteriaEvalChain # Import the base class for our custom evaluator.\n",
    "\n",
    "\n",
    "# Define our custom evaluator by inheriting from the base class.\n",
    "class CustomCriteriaEvalChain(LabeledCriteriaEvalChain):\n",
    "    def _get_eval_input(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        reference: Optional[str],\n",
    "        input: Optional[str],\n",
    "    ) -> dict:\n",
    "        # First, get the standard dictionary of inputs from the parent class.\n",
    "        raw = super()._get_eval_input(prediction, reference, input)\n",
    "        # This is the key step: we take the 'reference' (our code snippet) and execute it.\n",
    "        # The result of the execution replaces the code snippet in the dictionary.\n",
    "        # WARNING: This uses `eval`, which is a security risk with untrusted code.\n",
    "        raw[\"reference\"] = eval(raw[\"reference\"])\n",
    "        # Return the modified dictionary with the live, dereferenced answer.\n",
    "        return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c5533bc-fafe-4822-aa74-6f025ab0f730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/6rnp_mbx5914kx7qmmh7xzmw0000gn/T/ipykernel_80945/2037493188.py:22: UserWarning: Function evaluate_existing is in beta.\\n\n",
      "  chain_results = evaluate_existing(\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sparkling-suit-54' at:\\n\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/8cf28879-611e-4532-9641-d593f6bffa20/compare?selectedSessions=e3e8387c-65b9-4f0e-bd20-519c28731949\\n\n",
      "\\n\n",
      "\\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd65fb16109e4833b52b1eb7f082add1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate # Import the necessary evaluation functions.\n",
    "\n",
    "# Instantiate our custom evaluator. We'll use GPT-4 as the judge for high-quality grading.\n",
    "base_evaluator = CustomCriteriaEvalChain.from_llm(\n",
    "    criteria=\"correctness\", llm=ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n",
    ")\n",
    "\n",
    "\n",
    "# Define a helper function to prepare the data format that our evaluator expects.\n",
    "def prepare_inputs(run, example):\n",
    "    return {\n",
    "        \"prediction\": next(iter(run.outputs.values())), # Get the model's predicted output.\n",
    "        \"reference\": next(iter(example.outputs.values())), # Get the reference (our code snippet).\n",
    "        \"input\": example.inputs[\"question\"], # Get the original input question.\n",
    "    }\n",
    "\n",
    "\n",
    "# Wrap our custom evaluator in a LangChainStringEvaluator to make it compatible with the `evaluate` function.\n",
    "criteria_evaluator = LangChainStringEvaluator(\n",
    "    base_evaluator, prepare_data=prepare_inputs\n",
    ")\n",
    "# Run the evaluation.\n",
    "chain_results = evaluate(\n",
    "    predict, # The function representing our Q&A system.\n",
    "    data=dataset_name, # The name of our dataset in LangSmith.\n",
    "    evaluators=[criteria_evaluator], # The list of evaluators to apply.\n",
    "    # The pandas agent does not currently support parallel execution.\n",
    "    max_concurrency=1,\n",
    "    metadata={\n",
    "        \"time\": \"T1\", # Add metadata to tag this run as our first time point.\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3b41a-b0bd-4b78-8f6e-04ec7a01a467",
   "metadata": {},
   "source": [
    "### Re-evaluate After Data Changes\n",
    "\n",
    "While the Titanic dataset is static, we can simulate a data update in a real-world system. We will modify the DataFrame by duplicating all the rows and shuffling some of the columns. This will drastically change the correct answer to every question in our dataset.\n",
    "\n",
    "Because our dataset contains *instructions* for finding the answer, not the answers themselves, we can re-run the exact same evaluation on the new data and get a meaningful correctness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f25324d0-0dd4-4364-8d68-81c813ed99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a data update by doubling the number of rows.\n",
    "df_doubled = pd.concat([df, df], ignore_index=True)\n",
    "# Shuffle some of the columns to make the data changes less trivial.\n",
    "df_doubled[\"Age\"] = df_doubled[\"Age\"].sample(frac=1).reset_index(drop=True)\n",
    "df_doubled[\"Sex\"] = df_doubled[\"Sex\"].sample(frac=1).reset_index(drop=True)\n",
    "# Overwrite the original DataFrame with the new, modified data.\n",
    "df = df_doubled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-e7f8-a9b0-c1d2e3f4a5b6",
   "metadata": {},
   "source": [
    "Now, we run the evaluation again. Note that the code is identical to our first evaluation run, except for the metadata tag, which we'll change to `\"T2\"` to signify the second time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f67443-dec8-498a-b09d-aa7c09a67ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/6rnp_mbx5914kx7qmmh7xzmw0000gn/T/ipykernel_80945/266341347.py:1: UserWarning: Function evaluate is in beta.\\n\n",
      "  chain_results = evaluate(\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'perfect-sofa-52' at:\\n\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/8cf28879-611e-4532-9641-d593f6bffa20/compare?selectedSessions=06bb0be8-4b77-43e3-80b3-e2c0b67900f8\\n\n",
      "\\n\n",
      "\\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c551027fef634e09bec1d4708ab00a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-run the evaluation on the modified DataFrame.\n",
    "chain_results = evaluate(\n",
    "    predict, # The same Q&A system function.\n",
    "    data=dataset_name, # The same dataset of questions and code snippets.\n",
    "    evaluators=[criteria_evaluator], # The same custom evaluator.\n",
    "    max_concurrency=1, # The agent still doesn't support concurrent runs.\n",
    "    metadata={\n",
    "        \"time\": \"T2\", # Update the metadata to mark this as the second run.\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41aca1-fa40-4313-a34b-115a76c52a21",
   "metadata": {},
   "source": [
    "### Review the results\n",
    "\n",
    "Let's inspect the example for the question, \"How many male and female passengers were there?\". The table of linked runs clearly shows two different predictions for our two test runs (`T1` and `T2`).\n",
    "\n",
    "- In the first run, the agent correctly predicted 577 male and 314 female passengers.\n",
    "- In the second run, after we doubled the data, it correctly predicted 1154 male and 628 female passengers.\n",
    "\n",
    "**Both test runs were marked as correct**. This demonstrates that our evaluation setup is working perfectly. The agent's predictions changed to reflect the new data, and our evaluator correctly fetched the new ground truth, confirming that both answers were correct *at the time they were generated*.\n",
    "\n",
    "To be absolutely sure, we can inspect the traces of the evaluator itself. By clicking on the \"correctness\" feedback chips, we can see exactly what inputs the evaluator's LLM received. The screenshots below show the `reference` value that was passed to the LLM judge. You can see that for the `T1` run, the dereferenced value was `(577, 314)`, and for the `T2` run, it was `(1154, 628)`. This confirms our custom evaluator is successfully dereferencing the labels and fetching the live data before making its judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2d3d1-4f87-4969-b372-34030c45406e",
   "metadata": {},
   "source": [
    "### Limitations and Practical Considerations\n",
    "Dynamic ground truth introduces additional complexity and requires careful control to ensure reproducibility. External dependencies must be stable enough to support repeated evaluation, and changes in upstream systems can alter results over time.\n",
    "\n",
    "For this reason, dynamic evaluation is best used in conjunction with logging and versioning, so that changes in outcomes can be attributed to changes in the environment rather than to model behaviour alone.\n",
    "\n",
    "### Role in a Broader Evaluation Framework\n",
    "Within this project, dynamic ground truth addresses a class of failures that static benchmarks cannot capture. When combined with trajectory analysis and structured validation, it helps identify whether an agent failed because it misunderstood the task, queried the wrong information, or operated on outdated assumptions.\n",
    "\n",
    "This distinction is especially important for agentic systems that interact with external tools and evolving data sources.\n",
    "\n",
    "## Discussion\n",
    "As models are increasingly deployed in environments that change over time, evaluation methods must account for non-stationarity. Dynamic ground truth provides a principled way to do this, shifting evaluation from matching stored answers to verifying behaviour against the current state of the world."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
