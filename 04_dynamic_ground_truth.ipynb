{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce8c14f-0412-4d1b-a3fd-ad195daf3622",
   "metadata": {},
   "source": [
    "# Evaluating Q&A Systems with Dynamic Data\n",
    "\n",
    "====================================\n",
    "\n",
    "### 1. Theory: The Challenge of Evaluating on Dynamic Data\n",
    "\n",
    "In many real-world applications, a Q&A system doesn't operate on static documents; it connects to live, changing data sources like a production database or an external API. For example, a system answering \"How many users signed up today?\" needs to be correct even though the answer changes every minute. Standard evaluation, which relies on fixed, hand-labeled answers, breaks down in this scenario. A test that passes in the morning might fail in the afternoon simply because the data has changed.\n",
    "\n",
    "To solve this, we can borrow a classic computer science concept: **indirection**. Instead of storing the ground-truth answers (the labels) as static values in our dataset, we store *references* that tell us how to fetch the correct answer at the moment of evaluation. In this notebook, our \"references\" will be executable Python code snippets. \n",
    "\n",
    "This tutorial will walk you through the following steps:\n",
    "\n",
    "1.  **Create a Dataset**: We will build a dataset where the inputs are questions and the \"outputs\" are code snippets that can retrieve the live answer from a data source.\n",
    "2.  **Define the Q&A System**: We will set up a LangChain agent that can query a pandas DataFrame.\n",
    "3.  **Run Evaluation with a Custom Evaluator**: We will design a custom evaluator in LangSmith that, at runtime, executes the code snippet from the dataset to get the current ground-truth answer before comparing it to the model's prediction.\n",
    "4.  **Re-test the System Over Time**: We will simulate a change in the underlying data and re-run the evaluation to prove our system remains accurate.\n",
    "\n",
    "> **Note**: We use a simple CSV file and pandas DataFrame to simulate a dynamic data source. This is for illustrative purposes; in a real-world scenario, this could be a SQL database, a GraphQL API, or any other data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### 2. Prerequisites and Setup\n",
    "\n",
    "This tutorial requires OpenAI models and LangChain. First, we will configure our environment variables to connect to the necessary services.\n",
    "\n",
    "- **`LANGCHAIN_ENDPOINT`**: This URL tells LangChain to send all tracing data to the LangSmith platform.\n",
    "- **`LANGCHAIN_API_KEY`**: This is your secret key for authenticating with LangSmith.\n",
    "\n",
    "**Action Required**: You must replace `\"YOUR API KEY\"` with your actual key for this notebook to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b44ce-3c06-4029-b25d-dad3c02808fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system's environment variables.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the API endpoint for LangSmith.\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your personal LangSmith API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbe6e3-062e-41fd-901b-7fe9f2895a0a",
   "metadata": {},
   "source": [
    "Next, we install the required Python packages and set the OpenAI API key.\n",
    "\n",
    "- `langchain[openai]`: Installs the core LangChain library and integrations for OpenAI models.\n",
    "- `pandas`: The library for data manipulation and analysis, used here as our data source.\n",
    "\n",
    "**Action Required**: Replace `<YOUR-API-KEY>` with your actual OpenAI key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a536c5d-0b6b-45eb-8dcb-eabdf3caaae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%pip install' command installs python packages. '> /dev/null' suppresses the output.\n",
    "# %pip install -U \"langchain[openai]\" > /dev/null\n",
    "# %pip install pandas > /dev/null\n",
    "# The '%env' magic command sets an environment variable for the notebook session.\n",
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2587dd-19e4-4748-b211-8a97d0c7c5a5",
   "metadata": {},
   "source": [
    "## Step 1: Create a Dataset with Dynamic References\n",
    "\n",
    "We will use the classic Titanic dataset as our data source. The key difference in our approach is how we define the labels. Instead of calculating the answers beforehand and storing them as static values, we will store Python code snippets that can be executed on the DataFrame to get the correct answer.\n",
    "\n",
    "This is the principle of **indirection** in action. The label is not the answer itself, but a *recipe for finding the answer*. This ensures that our evaluation always compares against the most up-to-date data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d51bd9b-bea5-4670-a3a1-e24d853e2a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a list of tuples, where each tuple is a (question, code_snippet) pair.\n",
    "questions = [\n",
    "    (\"How many passengers were on the Titanic?\", \"len(df)\"),\n",
    "    (\"How many passengers survived?\", \"df['Survived'].sum()\"),\n",
    "    (\"What was the average age of the passengers?\", \"df['Age'].mean()\"),\n",
    "    (\"How many male and female passengers were there?\", \"df['Sex'].value_counts()\"),\n",
    "    (\"What was the average fare paid for the tickets?\", \"df['Fare'].mean()\"),\n",
    "    (\"How many passengers were in each class?\", \"df['Pclass'].value_counts()\"),\n",
    "    (\n",
    "        \"What was the survival rate for each gender?\",\n",
    "        \"df.groupby('Sex')['Survived'].mean()\",\n",
    "    ),\n",
    "    (\n",
    "        \"What was the survival rate for each class?\",\n",
    "        \"df.groupby('Pclass')['Survived'].mean()\",\n",
    "    ),\n",
    "    (\n",
    "        \"Which port had the most passengers embark from?\",\n",
    "        \"df['Embarked'].value_counts().idxmax()\",\n",
    "    ),\n",
    "    (\n",
    "        \"How many children under the age of 18 survived?\",\n",
    "        \"df[df['Age'] < 18]['Survived'].sum()\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72744a57-2d2d-4135-bbdd-48370aba5b33",
   "metadata": {},
   "source": [
    "Next, we'll create the dataset in LangSmith. This allows us to version our test cases, share them with team members, and associate multiple test runs with the same set of questions over time. We will upload our `questions` list, where the question is the input and the code snippet is the output (our dynamic label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd5b2c0-1999-4015-8f43-18f5c3edaff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid # Import the uuid library to generate unique identifiers.\n",
    "\n",
    "from langsmith import Client # Import the Client class to interact with LangSmith.\n",
    "\n",
    "client = Client() # Instantiate the LangSmith client.\n",
    "# Define a unique name for the dataset using a short hex code from a UUID.\n",
    "dataset_name = f\"Dynamic Titanic CSV {uuid.uuid4().hex[:4]}\"\n",
    "# Create the dataset on the LangSmith platform.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, # The name for the new dataset.\n",
    "    description=\"Test QA over CSV\", # An optional description for the dataset.\n",
    ")\n",
    "\n",
    "# Create all the examples in the dataset in a single API call for efficiency.\n",
    "client.create_examples(\n",
    "    # The inputs are a list of dictionaries, each with a 'question' key.\n",
    "    inputs=[{\"question\": example[0]} for example in questions],\n",
    "    # The outputs are a list of dictionaries, each with a 'code' key containing the reference snippet.\n",
    "    outputs=[{\"code\": example[1]} for example in questions],\n",
    "    dataset_id=dataset.id, # Link these examples to the dataset we just created.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acabdc82-d426-4432-a09d-1daa560f08ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Define the Q&A System\n",
    "\n",
    "With the dataset created, it's time to define our question-answering system. For this tutorial, we'll use a pre-built LangChain component: the **pandas dataframe agent**. This agent is specifically designed to answer questions about a pandas DataFrame by generating and executing Python code.\n",
    "\n",
    "First, we load the Titanic data into a DataFrame. Then, we create a constructor function for our agent that we can pass to the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc67277-1191-4409-9f00-67d8e55e2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Import the pandas library for data manipulation.\n",
    "\n",
    "# The URL of the raw CSV file for the Titanic dataset.\n",
    "titanic_path = \"https://raw.githubusercontent.com/jorisvandenbossche/pandas-tutorial/master/data/titanic.csv\"\n",
    "# Read the CSV data from the URL into a pandas DataFrame.\n",
    "df = pd.read_csv(titanic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4-f5a6-b7c8-d9e0-f1a2b3c4d5e6",
   "metadata": {},
   "source": [
    "Now, we define the `predict` function. This function will be our \"system under test\". For each run, it initializes a new pandas dataframe agent with our designated LLM and the current state of the DataFrame `df`. It then invokes the agent with the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8555151b-aa5d-480b-962f-a2790f0b7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate # Import prompt templates.\n",
    "from langchain_experimental.agents import create_pandas_dataframe_agent # Import the agent constructor.\n",
    "from langchain_openai import ChatOpenAI # Import the OpenAI chat model wrapper.\n",
    "\n",
    "# Initialize the LLM. We use a powerful model like GPT-4 for code generation tasks and set temperature to 0 for deterministic outputs.\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0.0)\n",
    "\n",
    "\n",
    "# Define the function to be evaluated.\n",
    "def predict(inputs: dict):\n",
    "    # Inside the function, create an instance of the pandas dataframe agent.\n",
    "    agent = create_pandas_dataframe_agent(agent_type=\"openai-tools\", llm=llm, df=df)\n",
    "    # Invoke the agent with the question from the input dictionary.\n",
    "    return agent.invoke({\"input\": inputs[\"question\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be5a59c2-7eac-441e-8dd5-a73562ac004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How many passengers were on the Titanic?',\n",
       " 'output': 'There were 891 passengers on the Titanic according to the dataframe.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run an example prediction to see the agent in action.\n",
    "predict({\"question\": \"How many passengers were on the Titanic?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e127f9-ff9c-4ab0-9860-6d6432c02651",
   "metadata": {},
   "source": [
    "## Step 3: Run Evaluation with a Custom Evaluator\n",
    "\n",
    "This is the most critical part of our setup. We need an evaluator that understands our dynamic labels. We'll create a custom evaluator by inheriting from `LabeledCriteriaEvalChain`. This base class is an LLM-powered evaluator that assesses a prediction based on a given criterion (e.g., \"correctness\") and a reference label.\n",
    "\n",
    "Our customization is simple but powerful: we will override the `_get_eval_input` method. This method is responsible for preparing the inputs that get passed to the evaluator's LLM. In our overridden version, we will first call the parent method to get the standard inputs, and then we will **execute the `reference` value (our code snippet) using Python's `eval()` function**. This replaces the code snippet with its live result.\n",
    "\n",
    "The result is that the evaluator's LLM never sees the code; it only sees the prediction and the freshly fetched, up-to-the-minute correct answer.\n",
    "\n",
    "> **Security Warning**: Using `eval()` on untrusted code is extremely dangerous as it can execute arbitrary commands. In this tutorial, we are only evaluating code that we have written ourselves in a controlled environment. **Never** use this `eval()` approach in a production system where the code snippets could come from untrusted users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b8b84e5-f977-4893-bda7-20a1248469e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional # Import typing hints.\n",
    "\n",
    "from langchain.evaluation.criteria.eval_chain import LabeledCriteriaEvalChain # Import the base class for our custom evaluator.\n",
    "\n",
    "\n",
    "# Define our custom evaluator by inheriting from the base class.\n",
    "class CustomCriteriaEvalChain(LabeledCriteriaEvalChain):\n",
    "    def _get_eval_input(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        reference: Optional[str],\n",
    "        input: Optional[str],\n",
    "    ) -> dict:\n",
    "        # First, get the standard dictionary of inputs from the parent class.\n",
    "        raw = super()._get_eval_input(prediction, reference, input)\n",
    "        # This is the key step: we take the 'reference' (our code snippet) and execute it.\n",
    "        # The result of the execution replaces the code snippet in the dictionary.\n",
    "        # WARNING: This uses `eval`, which is a security risk with untrusted code.\n",
    "        raw[\"reference\"] = eval(raw[\"reference\"])\n",
    "        # Return the modified dictionary with the live, dereferenced answer.\n",
    "        return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4-a5b6-c7d8-e9f0-a1b2c3d4e5f6",
   "metadata": {},
   "source": [
    "With our custom evaluator class defined, we can now configure and run the full evaluation using LangSmith's `evaluate` function. This will iterate through our dataset, run our `predict` function on each question, and use our `CustomCriteriaEvalChain` to score the correctness of each result against the live data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c5533bc-fafe-4822-aa74-6f025ab0f730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/6rnp_mbx5914kx7qmmh7xzmw0000gn/T/ipykernel_80945/2037493188.py:22: UserWarning: Function evaluate_existing is in beta.\\n\n",
      "  chain_results = evaluate_existing(\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sparkling-suit-54' at:\\n\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/8cf28879-611e-4532-9641-d593f6bffa20/compare?selectedSessions=e3e8387c-65b9-4f0e-bd20-519c28731949\\n\n",
      "\\n\n",
      "\\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd65fb16109e4833b52b1eb7f082add1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate # Import the necessary evaluation functions.\n",
    "\n",
    "# Instantiate our custom evaluator. We'll use GPT-4 as the judge for high-quality grading.\n",
    "base_evaluator = CustomCriteriaEvalChain.from_llm(\n",
    "    criteria=\"correctness\", llm=ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n",
    ")\n",
    "\n",
    "\n",
    "# Define a helper function to prepare the data format that our evaluator expects.\n",
    "def prepare_inputs(run, example):\n",
    "    return {\n",
    "        \"prediction\": next(iter(run.outputs.values())), # Get the model's predicted output.\n",
    "        \"reference\": next(iter(example.outputs.values())), # Get the reference (our code snippet).\n",
    "        \"input\": example.inputs[\"question\"], # Get the original input question.\n",
    "    }\n",
    "\n",
    "\n",
    "# Wrap our custom evaluator in a LangChainStringEvaluator to make it compatible with the `evaluate` function.\n",
    "criteria_evaluator = LangChainStringEvaluator(\n",
    "    base_evaluator, prepare_data=prepare_inputs\n",
    ")\n",
    "# Run the evaluation.\n",
    "chain_results = evaluate(\n",
    "    predict, # The function representing our Q&A system.\n",
    "    data=dataset_name, # The name of our dataset in LangSmith.\n",
    "    evaluators=[criteria_evaluator], # The list of evaluators to apply.\n",
    "    # The pandas agent does not currently support parallel execution.\n",
    "    max_concurrency=1,\n",
    "    metadata={\n",
    "        \"time\": \"T1\", # Add metadata to tag this run as our first time point.\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bd377-288e-4193-8db1-5e61ab4dc4c0",
   "metadata": {},
   "source": [
    "With that evaluation running, you can navigate to the linked project in LangSmith to review the agent's predictions and the feedback scores from our custom evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3b41a-b0bd-4b78-8f6e-04ec7a01a467",
   "metadata": {},
   "source": [
    "## Step 4: Re-evaluate After Data Changes\n",
    "\n",
    "Now, we'll demonstrate the power of our dynamic evaluation setup. While the Titanic dataset is static, we can simulate a data update in a real-world system. We will modify the DataFrame by duplicating all the rows and shuffling some of the columns. This will drastically change the correct answer to every question in our dataset.\n",
    "\n",
    "Because our dataset contains *instructions* for finding the answer, not the answers themselves, we can re-run the exact same evaluation on the new data and get a meaningful correctness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f25324d0-0dd4-4364-8d68-81c813ed99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a data update by doubling the number of rows.\n",
    "df_doubled = pd.concat([df, df], ignore_index=True)\n",
    "# Shuffle some of the columns to make the data changes less trivial.\n",
    "df_doubled[\"Age\"] = df_doubled[\"Age\"].sample(frac=1).reset_index(drop=True)\n",
    "df_doubled[\"Sex\"] = df_doubled[\"Sex\"].sample(frac=1).reset_index(drop=True)\n",
    "# Overwrite the original DataFrame with the new, modified data.\n",
    "df = df_doubled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-e7f8-a9b0-c1d2e3f4a5b6",
   "metadata": {},
   "source": [
    "Now, we run the evaluation again. Note that the code is identical to our first evaluation run, except for the metadata tag, which we'll change to `\"T2\"` to signify the second time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f67443-dec8-498a-b09d-aa7c09a67ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/6rnp_mbx5914kx7qmmh7xzmw0000gn/T/ipykernel_80945/266341347.py:1: UserWarning: Function evaluate is in beta.\\n\n",
      "  chain_results = evaluate(\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'perfect-sofa-52' at:\\n\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/8cf28879-611e-4532-9641-d593f6bffa20/compare?selectedSessions=06bb0be8-4b77-43e3-80b3-e2c0b67900f8\\n\n",
      "\\n\n",
      "\\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c551027fef634e09bec1d4708ab00a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-run the evaluation on the modified DataFrame.\n",
    "chain_results = evaluate(\n",
    "    predict, # The same Q&A system function.\n",
    "    data=dataset_name, # The same dataset of questions and code snippets.\n",
    "    evaluators=[criteria_evaluator], # The same custom evaluator.\n",
    "    max_concurrency=1, # The agent still doesn't support concurrent runs.\n",
    "    metadata={\n",
    "        \"time\": \"T2\", # Update the metadata to mark this as the second run.\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41aca1-fa40-4313-a34b-115a76c52a21",
   "metadata": {},
   "source": [
    "#### Review the results\n",
    "\n",
    "Now that you've tested twice on the \"changing\" data source, you can check out the results in LangSmith. If you navigate to the dataset's page and click on the \"Examples\" tab, you can select any question and see the predictions from both test runs side-by-side. \n",
    "\n",
    "![Examples Table Page](./img/dynamic_data_examples_list.png)\n",
    " \n",
    "Let's inspect the example for the question, \"How many male and female passengers were there?\". The table of linked runs clearly shows two different predictions for our two test runs (`T1` and `T2`).\n",
    "\n",
    "- In the first run, the agent correctly predicted 577 male and 314 female passengers.\n",
    "- In the second run, after we doubled the data, it correctly predicted 1154 male and 628 female passengers.\n",
    "\n",
    "Crucially, **both test runs were marked as correct**. This demonstrates that our evaluation setup is working perfectly. The agent's predictions changed to reflect the new data, and our evaluator correctly fetched the new ground truth, confirming that both answers were correct *at the time they were generated*.\n",
    "\n",
    "![Examples Page](./img/dynamic_data_example_page.png)\n",
    "\n",
    "To be absolutely sure, we can inspect the traces of the evaluator itself. By clicking on the \"correctness\" feedback chips, we can see exactly what inputs the evaluator's LLM received. The screenshots below show the `reference` value that was passed to the LLM judge. You can see that for the `T1` run, the dereferenced value was `(577, 314)`, and for the `T2` run, it was `(1154, 628)`. This confirms our custom evaluator is successfully dereferencing the labels and fetching the live data before making its judgment.\n",
    "\n",
    "![Evaluator Trace at T1](./img/dynamic_data_feedback_trace_t1.png)\n",
    "\n",
    "![Evaluator Trace at T2](./img/dynamic_data_feedback_trace_t2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2d3d1-4f87-4969-b372-34030c45406e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this walkthrough, you have learned a powerful technique for evaluating a Q&A system connected to a dynamic, evolving data store. By using a custom evaluator that dynamically fetches the ground-truth answer based on a static reference (a code snippet), you can create a robust, end-to-end testing process that remains valid even as your data changes.\n",
    "\n",
    "This approach directly tests the correctness of your system on up-to-date data and is excellent for periodic performance checks.\n",
    "\n",
    "However, it's important to be aware of the trade-offs. This method is less suitable for A/B testing two different prompts or models, as a change in the underlying data between the two test runs could confound the results. Additionally, as highlighted, the use of `eval` requires extreme caution and should only be done in a secure environment with trusted code.\n",
    "\n",
    "Other strategies to consider for this problem include:\n",
    "- **Freezing or Mocking Data**: For A/B tests, you can use a static, frozen snapshot of your data source. This ensures that both models are tested on the exact same data, providing a fair comparison. The snapshot should be updated periodically to remain representative of the production environment.\n",
    "- **Evaluating Query Generation**: Instead of testing the final answer, you could test the agent's ability to generate the *correct query* (e.g., the correct SQL or Python code). This isolates the agent's logic from the data itself but is a less end-to-end test.\n",
    "\n",
    "We hope this gives you a solid framework for thinking about and implementing evaluations for your own dynamic Q&A systems! \n",
    "\n",
    "If you have questions or suggestions, please let us know at support@langchain.dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869b8c8-9c8f-4ad4-81a7-890ed70c5250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
