{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a7184af-d54f-487d-ad7f-0f3274dc689b",
   "metadata": {},
   "source": [
    "## Component-wise Evaluation of Retrieval-Augmented Generation Systems\n",
    "\n",
    "### Motivation\n",
    "End-to-end evaluation of retrieval-augmented generation (RAG) systems often obscures the source of failures. An incorrect or unfaithful answer may arise from retrieval errors, generation errors, or interactions between the two. Without isolating components, it is difficult to determine whether a system failed because relevant information was unavailable, incorrectly retrieved, or improperly used.\n",
    "\n",
    "This notebook studies component-wise evaluation as a way to attribute failures more precisely within RAG pipelines.\n",
    "\n",
    "### Experimental Setup\n",
    "We evaluate the response generation component independently by supplying fixed, curated source documents to the generator. By holding retrieval constant, we can examine whether the model correctly conditions on provided evidence, adheres to source material, and avoids introducing unsupported claims.\n",
    "\n",
    "This setup allows us to separate failures of information access from failures of information use.\n",
    "\n",
    "### What Component-wise Evaluation Reveals\n",
    "Isolating the generator surfaces several important failure modes:\n",
    "- hallucinated content despite correct evidence being available,\n",
    "- partial use of retrieved information,\n",
    "- selective attention to salient but irrelevant passages,\n",
    "- confident synthesis that diverges subtly from source documents.\n",
    "\n",
    "These behaviours are often masked in end-to-end evaluations, where retrieval and generation errors are conflated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### Prerequisites and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c748b92-e590-408f-bd20-733dc79d643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%pip install' command installs python packages from the notebook.\n",
    "# -U flag ensures we get the latest versions.\n",
    "%pip install -U langchain openai anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c086f0-f1c4-4a55-a922-c926239de2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system.\n",
    "import uuid # Import the uuid library to generate unique identifiers.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint.\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your API key.\n",
    "uid = uuid.uuid4() # Generate a unique ID to keep dataset names unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a0309-48f8-4770-8b34-2b97eb85a247",
   "metadata": {},
   "source": [
    "### Create a Dataset with Fixed Context\n",
    "\n",
    "The key feature of this dataset is its structure. For each example, the `inputs` dictionary will contain both the user's `question` and a list of `documents`. This list of documents represents the fixed context that our response generator will use. The `outputs` dictionary contains the `label`, which is the ground-truth answer for the correctness check.\n",
    "\n",
    "The examples below are designed to test if the response generator can correctly extract information and whether it will ignore its pre-trained knowledge in favor of the provided (and sometimes counter-intuitive) context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f83f2e-76d1-4d86-9275-35bd61df014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple example dataset to illustrate the concept.\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"What's the company's total revenue for q2 of 2022?\",\n",
    "            # The 'documents' are part of the input for the component we are testing.\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"metadata\": {},\n",
    "                    \"page_content\": \"In q1 the lemonade company made $4.95. In q2 revenue increased by a sizeable amount to just over $2T dollars.\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            # The 'label' is the ground-truth answer for correctness evaluation.\n",
    "            \"label\": \"2 trillion dollars\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"Who is Lebron?\",\n",
    "            # This document provides a fictional, counter-intuitive context.\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"metadata\": {},\n",
    "                    \"page_content\": \"On Thursday, February 16, Lebron James was nominated as President of the United States.\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"label\": \"Lebron James is the President of the USA.\",\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf85a45-e100-4d28-a102-ec1d135f0ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client # Import the Client class to interact with LangSmith.\n",
    "\n",
    "client = Client() # Instantiate the LangSmith client.\n",
    "\n",
    "dataset_name = f\"Faithfulness Example - {uid}\" # Create a unique name for our dataset.\n",
    "dataset = client.create_dataset(dataset_name=dataset_name) # Create the dataset on the LangSmith platform.\n",
    "# Create the examples in the dataset.\n",
    "client.create_examples(\n",
    "    inputs=[e[\"inputs\"] for e in examples], # Pass the list of input dictionaries.\n",
    "    outputs=[e[\"outputs\"] for e in examples], # Pass the list of output dictionaries.\n",
    "    dataset_id=dataset.id, # Link these examples to the dataset we just created.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa0264-d24f-495a-b9f6-87ddf97aaeb6",
   "metadata": {},
   "source": [
    "### Define the Chain Component\n",
    "\n",
    "We'll show the full chain for context, but we will clearly separate the **`response_synthesizer`** component. This synthesizer is the specific part of the chain that we will be evaluating. It takes a dictionary containing `documents` and a `question` and generates the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6314168f-9530-476f-949b-d49c40db55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import chat_models, prompts # Import core LangChain components.\n",
    "from langchain_core.documents import Document # Import the Document class.\n",
    "from langchain_core.retrievers import BaseRetriever # Import the base retriever class.\n",
    "from langchain_core.runnables import RunnablePassthrough # Import a passthrough runnable.\n",
    "\n",
    "\n",
    "# This is a placeholder retriever to illustrate the full chain. It will not be used in our evaluation.\n",
    "class MyRetriever(BaseRetriever):\n",
    "    def _get_relevant_documents(self, query, *, run_manager):\n",
    "        return [Document(page_content=\"Example\")]\n",
    "\n",
    "\n",
    "# This is the specific component we will be evaluating.\n",
    "response_synthesizer = prompts.ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Respond using the following documents as context:\\n{documents}\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ") | chat_models.ChatAnthropic(model=\"claude-2\", max_tokens=1000) # We pipe the prompt to an LLM.\n",
    "\n",
    "# The full RAG chain is shown below for illustrative purposes only.\n",
    "chain = {\n",
    "    \"documents\": MyRetriever(),\n",
    "    \"qusetion\": RunnablePassthrough(),\n",
    "} | response_synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6087fa-432d-4a59-b023-29058b5ec6ea",
   "metadata": {},
   "source": [
    "### Define a Custom Faithfulness Evaluator\n",
    "\n",
    "To measure faithfulness, we need an evaluator that checks if the model's `prediction` is consistent with the provided `documents`. Standard evaluators assume the reference context comes from the `outputs` of a dataset example. In our case, the context (the documents) is in the `inputs`.\n",
    "\n",
    "To handle this, we'll create a custom `FaithfulnessEvaluator`. This class will wrap a standard LangChain scoring evaluator but will override the data mapping. It will tell the underlying evaluator to use:\n",
    "- The model's generation as the `prediction`.\n",
    "- The `question` from the run's inputs as the `input`.\n",
    "- The `documents` from the *example's inputs* as the `reference` context.\n",
    "\n",
    "This allows us to use an off-the-shelf LLM-based scoring mechanism with our custom dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8217e940-e6d0-4f08-bed7-41cda7a35ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import RunEvaluator, EvaluationResult # Import the base classes for custom evaluation.\n",
    "from langchain.evaluation import load_evaluator # Import a helper to load built-in evaluators.\n",
    "\n",
    "\n",
    "# Define our custom evaluator class, inheriting from RunEvaluator.\n",
    "class FaithfulnessEvaluator(RunEvaluator):\n",
    "    def __init__(self):\n",
    "        # Initialize a built-in 'labeled_score_string' evaluator.\n",
    "        # This evaluator uses an LLM to score a prediction on a 1-10 scale based on given criteria.\n",
    "        self.evaluator = load_evaluator(\n",
    "            \"labeled_score_string\",\n",
    "            criteria={\n",
    "                \"faithful\": \"How faithful is the submission to the reference context?\"\n",
    "            },\n",
    "            normalize_by=10, # Normalize the score to be between 0 and 1.\n",
    "        )\n",
    "\n",
    "    # This is the core method that LangSmith will call for each run.\n",
    "    def evaluate_run(self, run, example) -> EvaluationResult:\n",
    "        # Call the underlying evaluator's 'evaluate_strings' method with custom-mapped fields.\n",
    "        res = self.evaluator.evaluate_strings(\n",
    "            prediction=next(iter(run.outputs.values())), # The LLM's generated answer.\n",
    "            input=run.inputs[\"question\"], # The user's question.\n",
    "            # This is the key part: we use the 'documents' from the example's INPUTS as the reference context.\n",
    "            reference=str(example.inputs[\"documents\"]),\n",
    "        )\n",
    "        # Return the result in the standard EvaluationResult format.\n",
    "        return EvaluationResult(key=\"labeled_criteria:faithful\", **res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-e7f8-f9a0-b1c2d3e4f5a6",
   "metadata": {},
   "source": [
    "### Run the Evaluation\n",
    "\n",
    "Now we can run the evaluation. We will configure it to use two evaluators:\n",
    "1. The standard `\"qa\"` evaluator, which will measure correctness against the `label` in our dataset outputs.\n",
    "2. Our custom `FaithfulnessEvaluator`, which will measure how grounded the response is in the provided documents.\n",
    "\n",
    "We will pass the `response_synthesizer` directly as the system to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e56fe8-ae02-481d-b6f6-729e535c5e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-puzzled-texture-92' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/4d35dd98-d797-47ce-ae4b-608e96ddf6bf\n",
      "[------------------------------------------------->] 2/2"
     ]
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig # Import the evaluation configuration class.\n",
    "\n",
    "# Create an evaluation configuration.\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"qa\"], # Include the standard 'qa' correctness evaluator.\n",
    "    custom_evaluators=[FaithfulnessEvaluator()], # Include our custom faithfulness evaluator.\n",
    "    input_key=\"question\", # Tell the 'qa' evaluator to use the 'question' field from the inputs.\n",
    ")\n",
    "# Run the evaluation on the dataset.\n",
    "results = client.run_on_dataset(\n",
    "    llm_or_chain_factory=response_synthesizer, # The specific component to be tested.\n",
    "    dataset_name=dataset_name, # The name of our dataset in LangSmith.\n",
    "    evaluation=eval_config, # The evaluation configuration.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e74b5b-b8e2-433d-a9da-c56539152833",
   "metadata": {},
   "source": [
    "You can now review the results in LangSmith by clicking the link in the output above. You will see scores for both correctness (`qa`) and faithfulness (`labeled_criteria:faithful`). Inspecting the trace for the faithfulness evaluator will show how the LLM judged the response against the provided documents.\n",
    "\n",
    "[![](./img/example_score.png)](https://smith.langchain.com/public/9a4e6ee2-f26c-4bcd-a050-04766fbfd350/r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9286b-0463-4f81-a27b-b2e3b16955c2",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "Component-wise evaluation does not capture interactions between retrieval and generation that occur in full pipelines, and it does not assess retrieval quality itself. Its value lies in diagnosis rather than completeness: it provides clarity about where failures occur, not a holistic system score.\n",
    "\n",
    "### Role in a Broader Evaluation Framework\n",
    "Within this project, component-wise RAG evaluation complements structured validation, LLM-based judging, and trajectory analysis. When used together, these methods allow failures to be localised to specific stages of the pipeline, enabling targeted improvements rather than broad, undirected changes.\n",
    "\n",
    "### Discussion\n",
    "As RAG systems are increasingly deployed in settings where faithfulness matters, understanding whether models are using evidence correctly becomes as important as whether they can retrieve it. This notebook demonstrates how isolating components can turn opaque system failures into tractable research questions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
