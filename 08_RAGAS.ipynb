{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe90efd1",
   "metadata": {},
   "source": [
    "# RAG evaluation with RAGAS\n",
    "\n",
    "====================================\n",
    "\n",
    "### 1. Theory: A Deep Dive into RAG Evaluation with RAGAS\n",
    "\n",
    "**RAGAS** is a powerful, open-source framework specifically designed for evaluating Retrieval-Augmented Generation (RAG) pipelines. A RAG pipeline's quality depends on multiple factors: Did it retrieve the right information? Did it generate a faithful answer based on that information? Was the answer itself correct? RAGAS provides a suite of metrics to dissect and score each part of this process.\n",
    "\n",
    "This notebook demonstrates how to integrate these comprehensive RAGAS metrics into the **LangSmith** evaluation platform. This allows you to track, visualize, and compare sophisticated RAG metrics over time, all within a unified testing environment.\n",
    "\n",
    "We will evaluate a simple RAG application using the following RAGAS metrics:\n",
    "\n",
    "#### Generator-Focused Metrics\n",
    "- `answer_correctness` (Labeled): Measures the factual accuracy of the generated answer compared to a ground-truth answer. This requires a labeled dataset.\n",
    "- `faithfulness` (Reference-Free): Measures how much the generated answer is grounded in the provided context. It checks for hallucinations by breaking the answer into claims and verifying each one against the retrieved documents.\n",
    "\n",
    "#### Retriever-Focused Metrics\n",
    "- `context_relevancy` (Reference-Free): Measures how relevant the retrieved context is to the input question. It does this by identifying sentences in the context that are crucial for answering the question.\n",
    "- `context_recall` (Labeled): Measures the retriever's ability to fetch all the necessary information required to answer the question, based on a ground-truth answer.\n",
    "- `context_precision` (Labeled): Measures if the most relevant documents are ranked higher by the retriever. It assesses whether the truly important context appears at the top of the retrieval results.\n",
    "\n",
    "By using these metrics together, we can get a holistic view of our RAG system's performance, identifying whether poor results stem from the retriever, the generator, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3a99a6-6e1f-45ac-a090-b25a75c02a7c",
   "metadata": {},
   "source": [
    "## 2. Prerequisites\n",
    "\n",
    "First, we'll install the necessary Python packages and configure our environment variables to connect to LangSmith and the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d733a0-398f-43fc-a217-7146a082d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# The '%%capture --no-stderr' magic command prevents the output of this cell (except for errors) from being displayed.\n",
    "# The '%pip install' command installs python packages from the notebook.\n",
    "# -U flag ensures we get the latest versions of the specified libraries.\n",
    "%pip install -U langsmith ragas numpy openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb3ce85-ceb5-4b01-9205-b6c079469818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass # Import the getpass library to securely prompt for credentials.\n",
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Set the environment variable to enable LangSmith tracing.\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# Prompt for the LangSmith API key and set it as an environment variable.\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LANGCHAIN_API_KEY\")\n",
    "# Prompt for the OpenAI API key and set it as an environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28504e01",
   "metadata": {},
   "source": [
    "## 3. Dataset\n",
    "\n",
    "We will use a pre-existing public dataset hosted on LangSmith called **\"BaseCamp Q&A\"**. This dataset was created by scraping the 37signals (the makers of BaseCamp) employee handbook and then synthetically generating question-answer pairs based on the content. We will clone this dataset into our own LangSmith account to use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4306d5-78a0-42e4-b5b3-df7b5cacf81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith # Import the langsmith library.\n",
    "\n",
    "client = langsmith.Client() # Instantiate the LangSmith client.\n",
    "# The URL of the public dataset we want to use.\n",
    "dataset_url = (\n",
    "    \"https://smith.langchain.com/public/56fe54cd-b7d7-4d3b-aaa0-88d7a2d30931/d\"\n",
    ")\n",
    "dataset_name = \"BaseCamp Q&A\" # The name for our local copy of the dataset.\n",
    "# Use the client to clone the public dataset into your own LangSmith workspace.\n",
    "client.clone_public_dataset(dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18681df",
   "metadata": {},
   "source": [
    "## 4. Define your RAG Pipeline\n",
    "\n",
    "We will now build our RAG pipeline from scratch. This involves fetching the source documents, creating a retriever to search through them, and defining the main RAG bot that synthesizes answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-2e3f4a5b6c7d",
   "metadata": {},
   "source": [
    "### 4.1 Fetch Source Documents\n",
    "First, we download the raw markdown files from the BaseCamp handbook, which will serve as our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5effaff1-3451-440a-bdeb-702fbc8682be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io # Import the io module for in-memory binary streams.\n",
    "import os # Import the os module for interacting with the file system.\n",
    "import zipfile # Import the zipfile module to handle zip archives.\n",
    "\n",
    "import requests # Import the requests library to make HTTP requests.\n",
    "\n",
    "# The URL of the zip file containing our source documents.\n",
    "url = \"https://storage.googleapis.com/benchmarks-artifacts/basecamp-data/basecamp-data.zip\"\n",
    "\n",
    "# Fetch the content from the URL.\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "# Open the downloaded content as an in-memory zip file.\n",
    "with io.BytesIO(response.content) as zipped_file:\n",
    "    # Create a ZipFile object.\n",
    "    with zipfile.ZipFile(zipped_file, \"r\") as zip_ref:\n",
    "        # Extract all the contents into the current directory.\n",
    "        zip_ref.extractall()\n",
    "\n",
    "# Define the directory where the data was extracted.\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "# Initialize an empty list to store the documents.\n",
    "docs = []\n",
    "# Iterate through all files in the data directory.\n",
    "for filename in os.listdir(data_dir):\n",
    "    # Check if the file is a markdown file.\n",
    "    if filename.endswith(\".md\"):\n",
    "        # Open and read the file content.\n",
    "        with open(os.path.join(data_dir, filename), \"r\") as file:\n",
    "            # Append the file's name and content to our list of docs.\n",
    "            docs.append({\"file\": filename, \"content\": file.read()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6a0ce-19ae-40de-bd97-68037567d8cf",
   "metadata": {},
   "source": [
    "### 4.2 Create the Retriever\n",
    "Next, we create a simple in-memory vector store retriever. This component will be responsible for:\n",
    "1.  Converting all source documents into numerical vector embeddings.\n",
    "2.  When given a query, converting the query into a vector.\n",
    "3.  Finding the documents with embeddings most similar to the query's embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e4fb09-e412-49a5-b4b4-133ee0f1c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List # Import typing hints.\n",
    "\n",
    "import numpy as np # Import numpy for numerical operations.\n",
    "import openai # Import the openai library.\n",
    "from langsmith import traceable # Import the traceable decorator for LangSmith.\n",
    "\n",
    "\n",
    "# Define a custom class for our simple vector store retriever.\n",
    "class VectorStoreRetriever:\n",
    "    def __init__(self, docs: list, vectors: list, oai_client):\n",
    "        self._arr = np.array(vectors) # The numpy array of document vectors.\n",
    "        self._docs = docs # The list of original documents.\n",
    "        self._client = oai_client # The OpenAI client.\n",
    "\n",
    "    # A class method to create an instance asynchronously by embedding documents.\n",
    "    @classmethod\n",
    "    async def from_docs(cls, docs, oai_client):\n",
    "        # Generate embeddings for all document contents.\n",
    "        embeddings = await oai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\", input=[doc[\"content\"] for doc in docs]\n",
    "        )\n",
    "        # Extract the vector embeddings from the response.\n",
    "        vectors = [emb.embedding for emb in embeddings.data]\n",
    "        # Return a new instance of the class.\n",
    "        return cls(docs, vectors, oai_client)\n",
    "\n",
    "    # The traceable decorator ensures that calls to this method are logged to LangSmith.\n",
    "    @traceable\n",
    "    async def query(self, query: str, k: int = 5) -> List[dict]:\n",
    "        # Generate an embedding for the input query.\n",
    "        embed = await self._client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\", input=[query]\n",
    "        )\n",
    "        # Calculate similarity scores using matrix multiplication (dot product).\n",
    "        scores = np.array(embed.data[0].embedding) @ self._arr.T\n",
    "        # Find the indices of the top K most similar documents.\n",
    "        top_k_idx = np.argpartition(scores, -k)[-k:]\n",
    "        # Sort the top K indices by their scores in descending order.\n",
    "        top_k_idx_sorted = top_k_idx[np.argsort(-scores[top_k_idx])]\n",
    "        # Return the top K documents along with their similarity scores.\n",
    "        return [\n",
    "            {**self._docs[idx], \"similarity\": scores[idx]} for idx in top_k_idx_sorted\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4-f5a6-b7c8-d9e0-f2a2b3c4d5e6",
   "metadata": {},
   "source": [
    "### 4.3 Create the RAG Bot\n",
    "\n",
    "Now we define the main `NaiveRagBot` class. This class orchestrates the RAG process. Its `get_answer` method will first call the retriever to get relevant documents and then call an LLM with a prompt that includes these documents as context. \n",
    "\n",
    "A crucial detail is the format of the output. The RAGAS evaluators expect the final output to be a dictionary containing an `\"answer\"` key (the generated text) and a `\"contexts\"` key (a list of the document contents used). We must structure our output accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23a3852-32b1-48f4-95f3-af71d559a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable # Import the traceable decorator.\n",
    "from langsmith.wrappers import wrap_openai # Import the wrapper to instrument the OpenAI client.\n",
    "\n",
    "\n",
    "# Define the class for our RAG bot.\n",
    "class NaiveRagBot:\n",
    "    def __init__(self, retriever, model: str = \"gpt-4-turbo-preview\"):\n",
    "        self._retriever = retriever # The retriever instance.\n",
    "        # Wrapping the client with `wrap_openai` automatically instruments all calls to the OpenAI API for LangSmith tracing.\n",
    "        self._client = wrap_openai(openai.AsyncClient())\n",
    "        self._model = model # The name of the LLM to use.\n",
    "\n",
    "    # Decorate the main method with @traceable to create a root run in LangSmith.\n",
    "    @traceable\n",
    "    async def get_answer(self, question: str):\n",
    "        # 1. Retrieve relevant documents.\n",
    "        similar = await self._retriever.query(question)\n",
    "        # 2. Call the LLM with the retrieved context and the question.\n",
    "        response = await self._client.chat.completions.create(\n",
    "            model=self._model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    # The system prompt instructs the model on how to use the context.\n",
    "                    \"content\": \"You are a helpful AI assistant.\"\n",
    "                    \" Use the following docs to help answer the user's question.\\n\\n\"\n",
    "                    f\"## Docs\\n\\n{similar}\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # 3. Format the output to be compatible with RAGAS evaluators.\n",
    "        # This dictionary structure with 'answer' and 'contexts' keys is required.\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4-a5b6-c7d8-e9f0-c2d3e4f5a6b7",
   "metadata": {},
   "source": [
    "### 4.4 Instantiate and Test the Pipeline\n",
    "Now we create an instance of our retriever and RAG bot. We'll also do a quick test run to ensure it's working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d3a9f7-2ca2-495e-8cbc-5d276bcb9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronously create an instance of our retriever from the documents.\n",
    "retriever = await VectorStoreRetriever.from_docs(docs, openai.AsyncClient())\n",
    "# Create an instance of our RAG bot, passing the retriever to it.\n",
    "rag_bot = NaiveRagBot(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ee69f5-d0f2-4683-ad1e-e3a71c2aa18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided documents, at 37signals, employees are entitled to various forms of time off, including:\\n\\n1. **Paid Time Off (Vacation Time)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a single prediction with a sample question.\n",
    "response = await rag_bot.get_answer(\"How much time off do we get?\")\n",
    "# Display the first 150 characters of the answer to verify it's working.\n",
    "response[\"answer\"][:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92529864-e251-4b93-b968-b885c3d2f058",
   "metadata": {},
   "source": [
    "## 5. Evaluate.\n",
    "\n",
    "Now it's time to define our evaluators. RAGAS provides a variety of metrics that we can use. To integrate them with LangSmith, we simply wrap each desired RAGAS metric in LangChain's `EvaluatorChain`. This makes them compatible with the LangSmith evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca237aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig # Import the evaluation configuration class.\n",
    "from ragas.integrations.langchain import EvaluatorChain # Import the RAGAS LangChain wrapper.\n",
    "from ragas.metrics import ( # Import the specific metrics we want to use from RAGAS.\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    context_relevancy,\n",
    "    faithfulness,\n",
    ")\n",
    "\n",
    "# A list to hold our wrapped RAGAS evaluator chains.\n",
    "evaluators = [\n",
    "    # Wrap each RAGAS metric in an EvaluatorChain.\n",
    "    EvaluatorChain(metric)\n",
    "    for metric in [\n",
    "        answer_correctness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "    ]\n",
    "]\n",
    "# Create an evaluation configuration that uses our list of RAGAS evaluators.\n",
    "eval_config = RunEvalConfig(custom_evaluators=evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-e7f8-f9a0-e2d3e4f5a6b7",
   "metadata": {},
   "source": [
    "Finally, we run the evaluation. LangSmith's `arun_on_dataset` function will asynchronously run our `rag_bot` on every example in the dataset and then apply all the configured RAGAS evaluators to each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d43007aa-3e80-4039-b4e5-cff030bb89c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'back-bibliography-2' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/e468392e-e369-4066-99d4-dd05e186e992/compare?selectedSessions=bf005eaa-498d-47f7-a752-4bd260000c23\n",
      "\n",
      "View all tests for Dataset BaseCamp Q&A at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/e468392e-e369-4066-99d4-dd05e186e992\n",
      "[->                                                ] 1/21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfh/code/lc/langchain/libs/core/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------->                                 ] 7/21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid JSON response. Expected dictionary with key 'question'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----------------------->                          ] 10/21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid JSON response. Expected dictionary with key 'question'\n",
      "Invalid response format. Expected a list of dictionaries with keys 'verdict'\n",
      "Invalid JSON response. Expected dictionary with key 'question'\n",
      "Invalid JSON response. Expected dictionary with key 'Attributed'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------------------->                     ] 12/21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid JSON response. Expected dictionary with key 'question'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------>       ] 18/21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid JSON response. Expected dictionary with key 'question'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------------------------------------->     ] 19/21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid JSON response. Expected dictionary with key 'Attributed'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 21/21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid JSON response. Expected dictionary with key 'question'\n"
     ]
    }
   ],
   "source": [
    "# Asynchronously run the evaluation on the dataset.\n",
    "results = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name, # The name of the dataset to test against.\n",
    "    llm_or_chain_factory=rag_bot.get_answer, # A reference to the RAG bot's answer method.\n",
    "    evaluation=eval_config, # The evaluation configuration with our RAGAS metrics.\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
