{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33c67",
   "metadata": {},
   "source": [
    "## Pairwise Evaluation for Detecting Qualitative Differences\n",
    "\n",
    "### Motivation\n",
    "Absolute scoring of model outputs often obscures meaningful differences between systems, especially when performance metrics are close. In many cases, humans are better at answering relative questions—which output is better, and why?—than assigning absolute scores. This notebook studies pairwise comparison as an evaluation method for detecting subtle qualitative differences that aggregate metrics can miss.\n",
    "\n",
    "### Experimental Setup\n",
    "We present an evaluator with pairs of outputs generated by different systems (or configurations) for the same input and ask it to determine which response is preferable. The comparison criteria can be tailored to specific concerns, such as faithfulness, clarity, or adherence to constraints.\n",
    "\n",
    "By aggregating pairwise judgments across many examples, we can derive relative performance rankings without assuming a single absolute notion of correctness.\n",
    "\n",
    "### What Pairwise Comparison Reveals\n",
    "Pairwise evaluation is well-suited to detecting:\n",
    "- differences in reasoning quality when answers are superficially similar,\n",
    "- trade-offs between verbosity and precision,\n",
    "- subtle failures masked by high average scores,\n",
    "- regressions introduced by small system changes.\n",
    "\n",
    "These effects often remain invisible in scalar metrics but become clear when outputs are contrasted directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### Prerequisites and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c788783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:21.905597Z",
     "start_time": "2023-09-20T04:50:21.886104Z"
    }
   },
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint.\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e7425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:23.269252Z",
     "start_time": "2023-09-20T04:50:23.265495Z"
    }
   },
   "outputs": [],
   "source": [
    "# The '%pip install' command installs python packages from the notebook. '--quiet' suppresses the output.\n",
    "# %pip install -U \"langchain[openai]\" --quiet\n",
    "# %pip install chromadb --quiet\n",
    "# %pip install lxml --quiet\n",
    "# %pip install html2text --quiet\n",
    "# %pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac8079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:25.057984Z",
     "start_time": "2023-09-20T04:50:25.050240Z"
    }
   },
   "outputs": [],
   "source": [
    "# The '%env' magic command sets an environment variable for the notebook session.\n",
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ab6e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55e98a80-bf37-457e-b31d-952292e76c51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:27.908227Z",
     "start_time": "2023-09-20T04:50:27.903470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a list of tuples, where each tuple contains a (question, answer) pair.\n",
    "examples = [\n",
    "    (\n",
    "        \"What is LangChain?\",\n",
    "        \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How might I query for all runs in a project?\",\n",
    "        \"client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})\",\n",
    "    ),\n",
    "    (\n",
    "        \"What's a langsmith dataset?\",\n",
    "        \"A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How do I use a traceable decorator?\",\n",
    "        \"\"\"The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,\\\n",
    "import the required function, decorate your function, and then call the function. Below is an example:\n",
    "```python\n",
    "from langsmith.run_helpers import traceable\n",
    "@traceable(run_type=\"chain\") # or \"llm\", etc.\n",
    "def my_function(input_param):\n",
    "    # Function logic goes here\n",
    "    return output\n",
    "result = my_function(input_param)\n",
    "```\"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"Can I trace my Llama V2 llm?\",\n",
    "        \"So long as you are using one of LangChain's LLM implementations, all your calls can be traced\",\n",
    "    ),\n",
    "    (\n",
    "        \"Why do I have to set environment variables?\",\n",
    "        \"Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith.\"\n",
    "        \" While there are other ways to connect, environment variables tend to be the simplest way to configure your application.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How do I move my project between organizations?\",\n",
    "        \"LangSmith doesn't directly support moving projects between organizations.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edb7824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:28.911950Z",
     "start_time": "2023-09-20T04:50:28.903935Z"
    }
   },
   "outputs": [],
   "source": [
    "from langsmith import Client # Import the Client class to interact with LangSmith.\n",
    "\n",
    "client = Client() # Instantiate the LangSmith client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcd3690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:31.532293Z",
     "start_time": "2023-09-20T04:50:29.899253Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid # Import the uuid library to generate unique identifiers.\n",
    "\n",
    "dataset_name = f\"Retrieval QA Questions {str(uuid.uuid4())}\" # Create a unique name for the dataset.\n",
    "dataset = client.create_dataset(dataset_name=dataset_name) # Create the dataset on the LangSmith platform.\n",
    "# Iterate through our question-answer pairs.\n",
    "for q, a in examples:\n",
    "    # Create an example in our LangSmith dataset for each pair.\n",
    "    client.create_example(\n",
    "        inputs={\"question\": q}, outputs={\"answer\": a}, dataset_id=dataset.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976437f",
   "metadata": {},
   "source": [
    "The experiment we want to run is to see how the retriever's **chunk size** affects the quality of the final answer. We will create two versions of our RAG chain that are identical in every way except for the `chunk_size` and `chunk_overlap` used when splitting the source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fab721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:47.262606Z",
     "start_time": "2023-09-20T04:50:36.292760Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfh/.pyenv/versions/3.11.2/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\\n\n",
      "  warnings.warn(\\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader # A loader for recursively scraping a website.\n",
    "from langchain_community.document_transformers import Html2TextTransformer # A transformer to convert HTML to plain text.\n",
    "from langchain_community.vectorstores import Chroma # The Chroma vector store implementation.\n",
    "from langchain_text_splitters import TokenTextSplitter # A text splitter that splits based on token count.\n",
    "from langchain_openai import OpenAIEmbeddings # The class for using OpenAI's embedding models.\n",
    "\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\") # Initialize a loader for the LangSmith docs.\n",
    "doc_transformer = Html2TextTransformer() # Initialize the HTML to text transformer.\n",
    "raw_documents = api_loader.load() # Load the raw documents.\n",
    "transformed = doc_transformer.transform_documents(raw_documents) # Transform them into plain text.\n",
    "\n",
    "\n",
    "# Define a factory function to create a retriever based on a given text splitter.\n",
    "def create_retriever(transformed_documents, text_splitter):\n",
    "    documents = text_splitter.split_documents(transformed_documents) # Split the documents.\n",
    "    embeddings = OpenAIEmbeddings() # Initialize the embeddings model.\n",
    "    vectorstore = Chroma.from_documents(documents, embeddings) # Create the vector store.\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 4}) # Return the retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35f884-bb93-427d-a0ad-3858c449a1ea",
   "metadata": {},
   "source": [
    "We'll define a factory function for our chain. It will take a retriever as an argument, allowing us to easily create different chain versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2f266e9-e4de-42ad-b41e-99ace4dc5131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:47.264432Z",
     "start_time": "2023-09-20T04:50:47.263828Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime # Import datetime to include the current time in the prompt.\n",
    "from operator import itemgetter # Import itemgetter for convenient data routing.\n",
    "from langchain_core.output_parsers import StrOutputParser # Import the string output parser.\n",
    "from langchain_core.prompts import ChatPromptTemplate # Import the chat prompt template class.\n",
    "from langchain_openai import ChatOpenAI # Import the OpenAI chat model class.\n",
    "\n",
    "\n",
    "# Define a factory function that creates a RAG chain from a given retriever.\n",
    "def create_chain(retriever):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "                \" questions from LangSmith's documentation.\"\n",
    "                \" LangChain is a framework for building applications using large language models.\"\n",
    "                \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\",\n",
    "            ),\n",
    "            (\"system\", \"{context}\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    ).partial(time=str(datetime.now()))\n",
    "\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "    response_generator = prompt | model | StrOutputParser()\n",
    "    # Define the final chain using LangChain Expression Language (LCEL).\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\")\n",
    "            | retriever\n",
    "            | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "        }\n",
    "        | response_generator\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d02a25-2863-4c5c-81e8-00887247516d",
   "metadata": {},
   "source": [
    "`chain_1` will use larger document chunks, while `chain_2` will use smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1028d963-d84b-4759-a5ca-087b27065485",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T04:50:47.265315Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the text splitter for our first chain (large chunks).\n",
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "# Create the first retriever.\n",
    "retriever = create_retriever(transformed, text_splitter)\n",
    "\n",
    "# Create the first chain.\n",
    "chain_1 = create_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5765c938-c5f6-4ee4-be6b-90b7b341b683",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T04:50:47.266475Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the text splitter for our second chain (small chunks).\n",
    "text_splitter_2 = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "# Create the second retriever.\n",
    "retriever_2 = create_retriever(transformed, text_splitter_2)\n",
    "\n",
    "# Create the second chain.\n",
    "chain_2 = create_chain(retriever_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dddd61",
   "metadata": {},
   "source": [
    "We will run both `chain_1` and `chain_2` on our dataset and use a standard correctness evaluator (`cot_qa`) to score each response. This will give us an initial impression of their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62de3aea-638e-4add-998a-22eb21f6feaf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T04:50:47.267369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig # Import the evaluation configuration class.\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    # We will use the chain-of-thought Q&A correctness evaluator for a more robust grade.\n",
    "    evaluators=[\"cot_qa\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae51a83-6ae1-4fa6-a9d8-4e27bf03047f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T04:50:47.268056Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-new-goat-73' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/8097fed2-ca97-42b6-b71a-a24fe5e7c9d6\n",
      "[------------------------------------------------->] 7/7"
     ]
    }
   ],
   "source": [
    "# Run the evaluation for the first chain.\n",
    "results = client.run_on_dataset(\n",
    "    dataset_name=dataset_name, llm_or_chain_factory=chain_1, evaluation=eval_config\n",
    ")\n",
    "# Store the project name for later retrieval.\n",
    "project_name = results[\"project_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec375e76-7261-433b-9f02-f4750acf6f93",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T04:50:47.268715Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-large-stone-98' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/4f3b1676-741b-4fb1-bfc1-46ca630ac160\n",
      "[------------------------------------------------->] 7/7"
     ]
    }
   ],
   "source": [
    "# Run the evaluation for the second chain.\n",
    "results_2 = client.run_on_dataset(\n",
    "    dataset_name=dataset_name, llm_or_chain_factory=chain_2, evaluation=eval_config\n",
    ")\n",
    "# Store the project name for the second run.\n",
    "project_name_2 = results_2[\"project_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f50f74cc-fc86-4c60-8431-70695eba7aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:52.710833Z",
     "start_time": "2023-09-20T04:50:51.723217Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd # Import the pandas library.\n",
    "\n",
    "# Fetch all the runs from the first test project.\n",
    "runs_1 = list(client.list_runs(project_name=project_name, execution_order=1))\n",
    "# Fetch all the runs from the second test project.\n",
    "runs_2 = list(client.list_runs(project_name=project_name_2, execution_order=1))\n",
    "\n",
    "\n",
    "# Helper function to convert a list of runs into a DataFrame.\n",
    "def get_project_df(runs):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            # For each run, create a dictionary with its outputs and the average feedback scores.\n",
    "            {**run.outputs, **{k: v.get(\"avg\") for k, v in run.feedback_stats.items()}}\n",
    "            for run in runs\n",
    "        ],\n",
    "        # Use the reference example ID as the index for easy joining.\n",
    "        index=[run.reference_example_id for run in runs],\n",
    "    )\n",
    "\n",
    "\n",
    "runs_1_df = get_project_df(runs_1) # Create a DataFrame for the first run.\n",
    "runs_2_df = get_project_df(runs_2) # Create a DataFrame for the second run.\n",
    "# Join the two DataFrames on their index (the example ID).\n",
    "joined_df = runs_1_df.join(runs_2_df, lsuffix=\"_1\", rsuffix=\"_2\")\n",
    "# Reorder the columns for a clean side-by-side comparison.\n",
    "columns_1 = [col for col in joined_df.columns if col.endswith(\"_1\")]\n",
    "columns_2 = [col for col in joined_df.columns if col.endswith(\"_2\")]\n",
    "new_columns_order = [col for pair in zip(columns_1, columns_2) for col in pair]\n",
    "joined_df = joined_df[new_columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e3d81d-b16b-4ba2-bf4b-1118973e30d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:53.133194Z",
     "start_time": "2023-09-20T04:50:53.122750Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\\n\n",
       "<style scoped>\\n\n",
       "    .dataframe tbody tr th:only-of-type {\\n\n",
       "        vertical-align: middle;\\n\n",
       "    }\\n\n",
       "\\n\n",
       "    .dataframe tbody tr th {\\n\n",
       "        vertical-align: top;\\n\n",
       "    }\\n\n",
       "\\n\n",
       "    .dataframe thead th {\\n\n",
       "        text-align: right;\\n\n",
       "    }\\n\n",
       "</style>\\n\n",
       "<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\n",
       "  <thead>\\n\n",
       "    <tr style=\\\"text-align: right;\\\">\\n\n",
       "      <th></th>\\n\n",
       "      <th>output_1</th>\\n\n",
       "      <th>output_2</th>\\n\n",
       "      <th>COT Contextual Accuracy_1</th>\\n\n",
       "      <th>COT Contextual Accuracy_2</th>\\n\n",
       "    </tr>\\n\n",
       "  </thead>\\n\n",
       "  <tbody>\\n\n",
       "    <tr>\\n\n",
       "      <th>04a95258-4999-4abd-b1c3-0c5214130579</th>\\n\n",
       "      <td>LangChain is an open-source framework for buil...</td>\\n\n",
       "      <td>LangChain is an open-source framework for buil...</td>\\n\n",
       "      <td>1.0</td>\\n\n",
       "      <td>1.0</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>198d7039-72bc-4376-907a-87066d85275b</th>\\n\n",
       "      <td>To query for all runs in a project, you can us...</td>\\n\n",
       "      <td>To query for all runs in a project using LangS...</td>\\n\n",
       "      <td>0.0</td>\\n\n",
       "      <td>0.0</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>ea7b3f78-020e-410b-8e7b-bfdfa681a386</th>\\n\n",
       "      <td>A LangSmith dataset is a collection of input-o...</td>\\n\n",
       "      <td>A LangSmith dataset refers to a collection of ...</td>\\n\n",
       "      <td>1.0</td>\\n\n",
       "      <td>1.0</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>3cdd7b83-9a60-4c1b-b30b-dfa3fae69740</th>\\n\n",
       "      <td>To use a traceable decorator in LangSmith, you...</td>\\n\n",
       "      <td>To use the `traceable` decorator in LangSmith,...</td>\\n\n",
       "      <td>0.0</td>\\n\n",
       "      <td>0.0</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>5ec65a7d-70de-4494-b780-139550e301e5</th>\\n\n",
       "      <td>Yes, you can trace your Llama V2 LLM using Lan...</td>\\n\n",
       "      <td>Yes, you can trace your Llama V2 LLM using Lan...</td>\\n\n",
       "      <td>1.0</td>\\n\n",
       "      <td>1.0</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>7b74f055-3a2e-4b6f-a3d1-45eecfddcc63</th>\\n\n",
       "      <td>To move a project between organizations in Lan...</td>\\n\n",
       "      <td>To move a project between organizations in Lan...</td>\\n\n",
       "      <td>0.0</td>\\n\n",
       "      <td>0.0</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>f23253e5-c537-43f0-8059-38153090a884</th>\\n\n",
       "      <td>At LangChain, setting environment variables is...</td>\\n\n",
       "      <td>Setting environment variables is necessary in ...</td>\\n\n",
       "      <td>1.0</td>\\n\n",
       "      <td>1.0</td>\\n\n",
       "    </tr>\\n\n",
       "  </tbody>\\n\n",
       "</table>\\n\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               output_1  \\\\n\n",
       "04a95258-4999-4abd-b1c3-0c5214130579  LangChain is an open-source framework for buil...   \\\\n\n",
       "198d7039-72bc-4376-907a-87066d85275b  To query for all runs in a project, you can us...   \\\\n\n",
       "ea7b3f78-020e-410b-8e7b-bfdfa681a386  A LangSmith dataset is a collection of input-o...   \\\\n\n",
       "3cdd7b83-9a60-4c1b-b30b-dfa3fae69740  To use a traceable decorator in LangSmith, you...   \\\\n\n",
       "5ec65a7d-70de-4494-b780-139550e301e5  Yes, you can trace your Llama V2 LLM using Lan...   \\\\n\n",
       "7b74f055-3a2e-4b6f-a3d1-45eecfddcc63  To move a project between organizations in Lan...   \\\\n\n",
       "f23253e5-c537-43f0-8059-38153090a884  At LangChain, setting environment variables is...   \\\\n\n",
       "\\\\n\n",
       "                                                                               output_2  \\\\n\n",
       "04a95258-4999-4abd-b1c3-0c5214130579  LangChain is an open-source framework for buil...   \\\\n\n",
       "198d7039-72bc-4376-907a-87066d85275b  To query for all runs in a project using LangS...   \\\\n\n",
       "ea7b3f78-020e-410b-8e7b-bfdfa681a386  A LangSmith dataset refers to a collection of ...   \\\\n\n",
       "3cdd7b83-9a60-4c1b-b30b-dfa3fae69740  To use the `traceable` decorator in LangSmith,...   \\\\n\n",
       "5ec65a7d-70de-4494-b780-139550e301e5  Yes, you can trace your Llama V2 LLM using Lan...   \\\\n\n",
       "7b74f055-3a2e-4b6f-a3d1-45eecfddcc63  To move a project between organizations in Lan...   \\\\n\n",
       "f23253e5-c537-43f0-8059-38153090a884  Setting environment variables is necessary in ...   \\\\n\n",
       "\\\\n\n",
       "                                      COT Contextual Accuracy_1  \\\\\\n\n",
       "04a95258-4999-4abd-b1c3-0c5214130579                        1.0   \\\\n\n",
       "198d7039-72bc-4376-907a-87066d85275b                        0.0   \\\\n\n",
       "ea7b3f78-020e-410b-8e7b-bfdfa681a386                        1.0   \\\\n\n",
       "3cdd7b83-9a60-4c1b-b30b-dfa3fae69740                        0.0   \\\\n\n",
       "5ec65a7d-70de-4494-b780-139550e301e5                        1.0   \\\\n\n",
       "7b74f055-3a2e-4b6f-a3d1-45eecfddcc63                        0.0   \\\\n\n",
       "f23253e5-c537-43f0-8059-38153090a884                        1.0   \\\\n\n",
       "\\\\n\n",
       "                                      COT Contextual Accuracy_2  \\n\n",
       "04a95258-4999-4abd-b1c3-0c5214130579                        1.0  \\n\n",
       "198d7039-72bc-4376-907a-87066d85275b                        0.0  \\n\n",
       "ea7b3f78-020e-410b-8e7b-bfdfa681a386                        1.0  \\n\n",
       "3cdd7b83-9a60-4c1b-b30b-dfa3fae69740                        0.0  \\n\n",
       "5ec65a7d-70de-4494-b780-139550e301e5                        1.0  \\n\n",
       "7b74f055-3a2e-4b6f-a3d1-45eecfddcc63                        0.0  \\n\n",
       "f23253e5-c537-43f0-8059-38153090a884                        1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb19f4-1855-4e9c-a447-491625c66646",
   "metadata": {},
   "source": [
    "### Pairwise Evaluation\n",
    "\n",
    "Since the absolute scores were the same, we'll now run a pairwise evaluator to determine a preference between the two outputs for each question. We will define a helper function, `predict_preference`, to orchestrate this process for each example in our dataset. This function will:\n",
    "\n",
    "1.  Fetch the completed runs for a given example from both of our test projects (`project_a` and `project_b`).\n",
    "2.  **Randomize the order** of the two predictions (A and B). This is a crucial step to mitigate **positional bias**, where LLM judges may tend to prefer the first option they see.\n",
    "3.  Call a pairwise evaluation chain, which asks an LLM to state its preference (A or B).\n",
    "4.  Log the preference as feedback to the corresponding runs in LangSmith. The preferred run gets a score of `1`, and the other gets `0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf7c70f3-9c8f-436d-85ca-5a398c3b1369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random # Import the random module for shuffling.\n",
    "import logging # Import the logging module to handle potential errors.\n",
    "\n",
    "\n",
    "# A helper function to fetch a specific run and its prediction.\n",
    "def _get_run_and_prediction(example_id, project_name):\n",
    "    # List runs, filtering by the reference example and project.\n",
    "    run = next(\n",
    "        client.list_runs(\n",
    "            reference_example_id=example_id,\n",
    "            project_name=project_name,\n",
    "            execution_order=1,\n",
    "        )\n",
    "    )\n",
    "    # Extract the output from the run.\n",
    "    prediction = next(iter(run.outputs.values()))\n",
    "    return run, prediction\n",
    "\n",
    "\n",
    "# A helper function to log the preference feedback to LangSmith.\n",
    "def _log_feedback(run_ids):\n",
    "    # The 'preference' key is used. The preferred run gets a score of 1, the other gets 0.\n",
    "    for score, run_id in enumerate(run_ids):\n",
    "        client.create_feedback(run_id, key=\"preference\", score=score)\n",
    "\n",
    "\n",
    "# The main function to predict and log preference for a single example.\n",
    "def predict_preference(example, project_a, project_b, eval_chain):\n",
    "    example_id = example.id # Get the ID of the current example.\n",
    "    print(example) # Print the example for progress tracking.\n",
    "    # Fetch the runs and predictions for both projects (A and B).\n",
    "    run_a, pred_a = _get_run_and_prediction(example_id, project_a)\n",
    "    run_b, pred_b = _get_run_and_prediction(example_id, project_b)\n",
    "    # Prepare the inputs for the evaluator.\n",
    "    input_, answer = example.inputs[\"question\"], example.outputs[\"answer\"]\n",
    "    result = {\"input\": input_, \"answer\": answer, \"A\": pred_a, \"B\": pred_b}\n",
    "\n",
    "    # Randomly swap A and B to mitigate positional bias in the LLM judge.\n",
    "    if random.random() < 0.5:\n",
    "        result[\"A\"], result[\"B\"] = result[\"B\"], result[\"A\"]\n",
    "        run_a, run_b = run_b, run_a\n",
    "    try:\n",
    "        # Call the pairwise evaluator.\n",
    "        eval_res = eval_chain.evaluate_string_pairs(\n",
    "            prediction=result[\"A\"],\n",
    "            prediction_b=result[\"B\"],\n",
    "            input=input_,\n",
    "            reference=answer,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Log a warning if the evaluator fails.\n",
    "        logging.warning(e)\n",
    "        return result\n",
    "\n",
    "    # If the evaluator returns a 'None' value (e.g., a tie), we don't log feedback.\n",
    "    if eval_res[\"value\"] is None:\n",
    "        return result\n",
    "\n",
    "    # Determine which run was preferred.\n",
    "    preferred_run = (run_a.id, \"A\") if eval_res[\"value\"] == \"A\" else (run_b.id, \"B\")\n",
    "    runner_up_run = (run_b.id, \"B\") if eval_res[\"value\"] == \"A\" else (run_a.id, \"A\")\n",
    "    # Log the feedback (0 for runner-up, 1 for preferred).\n",
    "    _log_feedback((runner_up_run[0], preferred_run[0]))\n",
    "    # Add the preference to our results dictionary.\n",
    "    result[\"Preferred\"] = preferred_run[1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a51df-616f-4f3e-9106-a5f7669b9090",
   "metadata": {},
   "source": [
    "We will use LangChain's off-the-shelf `labeled_pairwise_string` evaluator. By default, this evaluator asks the LLM judge to choose its preference based on helpfulness, relevance, correctness, and depth. For a real application, you would likely want to customize these criteria to match your specific goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2e1f12e-ed28-46ce-b09b-37088c475bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator # Import the evaluator loading helper.\n",
    "\n",
    "# Load the pre-built labeled pairwise string evaluator.\n",
    "pairwise_evaluator = load_evaluator(\"labeled_pairwise_string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4-f5a6-b7c8-d9e0-f3a3b4c5d6e7",
   "metadata": {},
   "source": [
    "Now we'll set up a runnable to execute our `predict_preference` function across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f65e8572-18a3-46b3-936a-38e566b77922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools # Import functools to create partial functions.\n",
    "from langchain_core.runnables import RunnableLambda # Import RunnableLambda to wrap our function.\n",
    "\n",
    "\n",
    "# Create a partial function with our project names and evaluator pre-filled.\n",
    "eval_func = functools.partial(\n",
    "    predict_preference,\n",
    "    project_a=project_name,\n",
    "    project_b=project_name_2,\n",
    "    eval_chain=pairwise_evaluator,\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap our partial function in a RunnableLambda to get access to the convenient .batch() method.\n",
    "runnable = RunnableLambda(eval_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16969ec1-d27c-4f8e-9624-8c3f25c51f8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:45:43.550214Z",
     "start_time": "2023-09-20T04:45:43.459644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_id=UUID('29addcf7-2be5-4320-bae7-10f9635d29e3') inputs={'question': 'How do I move my project between organizations?'} outputs={'answer': \"LangSmith doesn't directly support moving projects between organizations.\"} id=UUID('7b74f055-3a2e-4b6f-a3d1-45eecfddcc63') created_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 588671) modified_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 645828) runs=[]dataset_id=UUID('29addcf7-2be5-4320-bae7-10f9635d29e3') inputs={'question': 'How do I use a traceable decorator?'} outputs={'answer': 'The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,import the required function, decorate your function, and then call the function. Below is an example:\\n```python\\nfrom langsmith.run_helpers import traceable\\n@traceable(run_type=\"chain\") # or \"llm\", etc.\\ndef my_function(input_param):\\n    # Function logic goes here\\n    return output\\nresult = my_function(input_param)\\n```'} id=UUID('3cdd7b83-9a60-4c1b-b30b-dfa3fae69740') created_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 266140) modified_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 325103) runs=[]\\n\n",
      "dataset_id=UUID('29addcf7-2be5-4320-bae7-10f9635d29e3') inputs={'question': 'What is LangChain?'} outputs={'answer': 'LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.'} id=UUID('04a95258-4999-4abd-b1c3-0c5214130579') created_at=datetime.datetime(2023, 10, 23, 6, 12, 26, 903170) modified_at=datetime.datetime(2023, 10, 23, 6, 12, 26, 963093) runs=[]\\n\n",
      "dataset_id=UUID('29addcf7-2be5-4320-bae7-10f9635d29e3') inputs={'question': 'Can I trace my Llama V2 llm?'} outputs={'answer': \"So long as you are using one of LangChain's LLM implementations, all your calls can be traced\"} id=UUID('5ec65a7d-70de-4494-b780-139550e301e5') created_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 363825) modified_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 435359) runs=[]\\n\n",
      "\\n\n",
      "dataset_id=UUID('29addcf7-2be5-4320-bae7-10f9635d29e3') inputs={'question': 'Why do I have to set environment variables?'} outputs={'answer': 'Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith. While there are other ways to connect, environment variables tend to be the simplest way to configure your application.'} id=UUID('f23253e5-c537-43f0-8059-38153090a884') created_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 485675) modified_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 547965) runs=[]\\n\n",
      "dataset_id=UUID('29addcf7-2be5-4320-bae7-10f9635d29e3') inputs={'question': \"What's a langsmith dataset?\"} outputs={'answer': 'A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.'} id=UUID('ea7b3f78-020e-410b-8e7b-bfdfa681a386') created_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 120041) modified_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 213155) runs=[]\\n\n",
      "dataset_id=UUID('29addcf7-2be5-4320-bae7-10f9635d29e3') inputs={'question': 'How might I query for all runs in a project?'} outputs={'answer': \"client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})\"} id=UUID('198d7039-72bc-4376-907a-87066d85275b') created_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 26145) modified_at=datetime.datetime(2023, 10, 23, 6, 12, 27, 80568) runs=[]\\n"
     ]
    }
   ],
   "source": [
    "# Fetch the list of examples from our dataset.\n",
    "examples = list(client.list_examples(dataset_id=dataset.id))\n",
    "# Run the evaluation in a batch across all examples.\n",
    "values = runnable.batch(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46c7d7f7-9e26-4439-8a2c-93f6d89741ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\\n\n",
       "<style scoped>\\n\n",
       "    .dataframe tbody tr th:only-of-type {\\n\n",
       "        vertical-align: middle;\\n\n",
       "    }\\n\n",
       "\\n\n",
       "    .dataframe tbody tr th {\\n\n",
       "        vertical-align: top;\\n\n",
       "    }\\n\n",
       "\\n\n",
       "    .dataframe thead th {\\n\n",
       "        text-align: right;\\n\n",
       "    }\\n\n",
       "</style>\\n\n",
       "<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\n",
       "  <thead>\\n\n",
       "    <tr style=\\\"text-align: right;\\\">\\n\n",
       "      <th></th>\\n\n",
       "      <th>input</th>\\n\n",
       "      <th>answer</th>\\n\n",
       "      <th>A</th>\\n\n",
       "      <th>B</th>\\n\n",
       "      <th>Preferred</th>\\n\n",
       "    </tr>\\n\n",
       "  </thead>\\n\n",
       "  <tbody>\\n\n",
       "    <tr>\\n\n",
       "      <th>0</th>\\n\n",
       "      <td>How do I move my project between organizations?</td>\\n\n",
       "      <td>LangSmith doesn't directly support moving proj...</td>\\n\n",
       "      <td>To move a project between organizations in Lan...</td>\\n\n",
       "      <td>To move a project between organizations in Lan...</td>\\n\n",
       "      <td>NaN</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>1</th>\\n\n",
       "      <td>Why do I have to set environment variables?</td>\\n\n",
       "      <td>Environment variables can tell your LangChain ...</td>\\n\n",
       "      <td>Setting environment variables is necessary in ...</td>\\n\n",
       "      <td>At LangChain, setting environment variables is...</td>\\n\n",
       "      <td>A</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>2</th>\\n\n",
       "      <td>Can I trace my Llama V2 llm?</td>\\n\n",
       "      <td>So long as you are using one of LangChain's LL...</td>\\n\n",
       "      <td>Yes, you can trace your Llama V2 LLM using Lan...</td>\\n\n",
       "      <td>Yes, you can trace your Llama V2 LLM using Lan...</td>\\n\n",
       "      <td>B</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>3</th>\\n\n",
       "      <td>How do I use a traceable decorator?</td>\\n\n",
       "      <td>The traceable decorator is available in the la...</td>\\n\n",
       "      <td>To use the `traceable` decorator in LangSmith,...</td>\\n\n",
       "      <td>To use a traceable decorator in LangSmith, you...</td>\\n\n",
       "      <td>A</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>4</th>\\n\n",
       "      <td>What's a langsmith dataset?</td>\\n\n",
       "      <td>A LangSmith dataset is a collection of example...</td>\\n\n",
       "      <td>A LangSmith dataset is a collection of input-o...</td>\\n\n",
       "      <td>A LangSmith dataset refers to a collection of ...</td>\\n\n",
       "      <td>A</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>5</th>\\n\n",
       "      <td>How might I query for all runs in a project?</td>\\n\n",
       "      <td>client.list_runs(project_name='my-project-name...</td>\\n\n",
       "      <td>To query for all runs in a project, you can us...</td>\\n\n",
       "      <td>To query for all runs in a project using LangS...</td>\\n\n",
       "      <td>A</td>\\n\n",
       "    </tr>\\n\n",
       "    <tr>\\n\n",
       "      <th>6</th>\\n\n",
       "      <td>What is LangChain?</td>\\n\n",
       "      <td>LangChain is an open-source framework for buil...</td>\\n\n",
       "      <td>LangChain is an open-source framework for buil...</td>\\n\n",
       "      <td>LangChain is an open-source framework for buil...</td>\\n\n",
       "      <td>A</td>\\n\n",
       "    </tr>\\n\n",
       "  </tbody>\\n\n",
       "</table>\\n\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input  \\\\n\n",
       "0  How do I move my project between organizations?   \\\\n\n",
       "1      Why do I have to set environment variables?   \\\\n\n",
       "2                     Can I trace my Llama V2 llm?   \\\\n\n",
       "3              How do I use a traceable decorator?   \\\\n\n",
       "4                      What's a langsmith dataset?   \\\\n\n",
       "5     How might I query for all runs in a project?   \\\\n\n",
       "6                               What is LangChain?   \\\\n\n",
       "\\\\n\n",
       "                                              answer  \\\\n\n",
       "0  LangSmith doesn't directly support moving proj...   \\\\n\n",
       "1  Environment variables can tell your LangChain ...   \\\\n\n",
       "2  So long as you are using one of LangChain's LL...   \\\\n\n",
       "3  The traceable decorator is available in the la...   \\\\n\n",
       "4  A LangSmith dataset is a collection of example...   \\\\n\n",
       "5  client.list_runs(project_name='my-project-name...   \\\\n\n",
       "6  LangChain is an open-source framework for buil...   \\\\n\n",
       "\\\\n\n",
       "                                                   A  \\\\n\n",
       "0  To move a project between organizations in Lan...   \\\\n\n",
       "1  Setting environment variables is necessary in ...   \\\\n\n",
       "2  Yes, you can trace your Llama V2 LLM using Lan...   \\\\n\n",
       "3  To use the `traceable` decorator in LangSmith,...   \\\\n\n",
       "4  A LangSmith dataset is a collection of input-o...   \\\\n\n",
       "5  To query for all runs in a project, you can us...   \\\\n\n",
       "6  LangChain is an open-source framework for buil...   \\\\n\n",
       "\\\\n\n",
       "                                                   B Preferred  \\n\n",
       "0  To move a project between organizations in Lan...       NaN  \\n\n",
       "1  At LangChain, setting environment variables is...         A  \\n\n",
       "2  Yes, you can trace your Llama V2 LLM using Lan...         B  \\n\n",
       "3  To use a traceable decorator in LangSmith, you...         A  \\n\n",
       "4  A LangSmith dataset refers to a collection of ...         A  \\n\n",
       "5  To query for all runs in a project using LangS...         A  \\n\n",
       "6  LangChain is an open-source framework for buil...         A  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # Import pandas.\n",
    "\n",
    "df = pd.DataFrame(values) # Create a DataFrame from our evaluation results.\n",
    "df.head(10) # Display the first 10 rows of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ec77a-67b3-48e8-9cde-c9022641f245",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "Pairwise judgments depend on the choice of evaluator and comparison criteria, and they do not scale linearly with the number of systems being compared. They are best used selectively, when fine-grained distinctions matter.\n",
    "\n",
    "### Role in a Broader Evaluation Framework\n",
    "In this project, pairwise comparison serves as a qualitative lens, complementing quantitative metrics. It is particularly useful when diagnosing regressions or validating whether changes that improve one metric actually improve overall behaviour.\n",
    "\n",
    "## Discussion\n",
    "Pairwise evaluation reframes assessment from scoring to preference, aligning more closely with how humans perceive quality. This notebook demonstrates how relative judgments can surface differences that absolute metrics systematically smooth over.\n",
    "\n",
    "This method can be enhanced by:\n",
    "\n",
    "- **Ensembling**: Using multiple LLM judges and taking a majority vote to get a more robust preference score.\n",
    "- **Continuous Scores**: Instructing the model to output a continuous score (e.g., from 1-10) for each response instead of just a binary preference.\n",
    "- **Custom Criteria**: Modifying the evaluator's prompt to judge based on criteria that are most important to your application, such as conciseness, tone, or creativity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
