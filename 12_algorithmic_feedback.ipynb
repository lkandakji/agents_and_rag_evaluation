{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bddb3342-ae12-4d6b-98d1-21cb0674c1d0",
   "metadata": {},
   "source": [
    "## Batch Feedback and Post-Hoc Evaluation at Scale\n",
    "\n",
    "### Motivation\n",
    "Not all evaluation needs to happen synchronously with model execution. In many production settings, large volumes of traces are generated continuously, and the primary challenge is enriching those traces with diagnostic signals after the fact. This notebook explores algorithmic feedback pipelines for applying evaluation functions in batch to completed runs.\n",
    "\n",
    "### Experimental Setup\n",
    "We apply automated evaluators to stored traces, attaching scores and annotations programmatically. This enables large-scale analysis without interrupting live systems and allows evaluation logic to evolve independently of model deployment.\n",
    "\n",
    "The approach supports scheduled re-evaluation as metrics or research questions change.\n",
    "\n",
    "### What Algorithmic Feedback Enables\n",
    "Batch feedback pipelines are particularly useful for:\n",
    "- retrospective analysis of system behaviour,\n",
    "- dataset curation for further study,\n",
    "- identifying rare but systematic failure patterns,\n",
    "- tracking longitudinal trends across deployments.\n",
    "\n",
    "By decoupling evaluation from execution, this approach makes it feasible to analyse behaviour at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### Prerequisites and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f496bc2-d9c6-46ea-be19-98ff2dd2800c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_HUB_API_URL\"] = \"https://api.hub.langchain.com\"\n",
    "# Update with your Hub API key\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = \"YOUR API KEY\"\n",
    "# Change to the project name you want to add feedback to.\n",
    "project_name = \"YOUR PROJECT NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edcc9b1-0c2c-4092-a354-0999ad1ef4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client # Import the LangSmith client.\n",
    "from datetime import datetime # Import the datetime module.\n",
    "\n",
    "client = Client() # Instantiate the client.\n",
    "# Define a list of example input/output pairs.\n",
    "example_data = [\n",
    "    (\"Who trained Llama-v2?\", \"I'm sorry, but I don't have that information.\"),\n",
    "    (\n",
    "        \"When did langchain first announce the hub?\",\n",
    "        \"LangChain first announced the LangChain Hub on September 5, 2023.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What's LangSmith?\",\n",
    "        \"LangSmith is a platform developed by LangChain for building production-grade LLM (Language Model) applications. It allows you to debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework. LangSmith seamlessly integrates with LangChain's open-source framework called LangChain, which is widely used for building applications with LLMs.\\n\\nLangSmith provides full visibility into model inputs and outputs at every step in the chain of events, making it easier to debug and analyze the behavior of LLM applications. It has been tested with early design partners and on internal workflows, and it has been found to help teams in various ways.\\n\\nYou can find more information about LangSmith on the official LangSmith documentation [here](https://docs.smith.langchain.com/). Additionally, you can read about the announcement of LangSmith as a unified platform for debugging and testing LLM applications [here](https://blog.langchain.dev/announcing-langsmith/).\",\n",
    "    ),\n",
    "    (\n",
    "        \"What is the langsmith cookbook?\",\n",
    "        \"I'm sorry, but I couldn't find any information about the \\\"Langsmith Cookbook\\\". It's possible that it may not be a well-known cookbook or it may not exist. Could you provide more context or clarify the name?\",\n",
    "    ),\n",
    "    (\n",
    "        \"What is LangChain?\",\n",
    "        \"I'm sorry, but I couldn't find any information about \\\"LangChain\\\". Could you please provide more context or clarify your question?\",\n",
    "    ),\n",
    "    (\"When was Llama-v2 released?\", \"Llama-v2 was released on July 18, 2023.\"),\n",
    "]\n",
    "\n",
    "# Loop through the example data to create runs in your project.\n",
    "for input_, output_ in example_data:\n",
    "    client.create_run(\n",
    "        name=\"ExampleRun\", # The name of the run.\n",
    "        run_type=\"chain\", # The type of the run.\n",
    "        inputs={\"input\": input_}, # The inputs to the run.\n",
    "        outputs={\"output\": output_}, # The outputs of the run.\n",
    "        project_name=project_name, # The project to associate the run with.\n",
    "        end_time=datetime.utcnow(), # The end time of the run.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bf771-6da1-4a98-840f-8800f07a8c9e",
   "metadata": {},
   "source": [
    "### Select Runs to Evaluate\n",
    "\n",
    "The first step in our feedback pipeline is to select the runs we want to score. The LangSmith client's `list_runs` method provides a powerful way to filter runs. You can filter by project, time, presence of errors, metadata tags, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9216ab8-4a0e-438b-80c5-50a2427df910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the current time and set it to midnight UTC.\n",
    "midnight = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# Fetch the list of runs from the specified project.\n",
    "runs = list(\n",
    "    client.list_runs(\n",
    "        project_name=project_name, # Filter by the project name.\n",
    "        execution_order=1, # Fetch in chronological order.\n",
    "        start_time=midnight, # Filter for runs that started after midnight.\n",
    "        error=False # Filter for runs that completed successfully.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efb84a-c84a-4903-95a8-e6c0d1e47fa4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Feedback Logic\n",
    "\n",
    "#### Example A: Simple Text Statistics\n",
    "\n",
    "First, we'll show how to apply a simple, non-LLM algorithm. We will use the `textstat` library to compute various readability scores (like Flesch reading ease) for the *input* to each run. This can be useful for understanding the complexity of user queries your application is receiving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78fadbbe-d641-4a37-abd8-e1a57dd1f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\\n"
     ]
    }
   ],
   "source": [
    "# Install the textstat library.\n",
    "%pip install textstat --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97b2d53f-6a4c-4e08-bc8e-93ebe28574d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textstat # Import the textstat library.\n",
    "from langsmith.schemas import Run, Example # Import the Run and Example schemas.\n",
    "from langchain_core.runnables import RunnableLambda # Import RunnableLambda for batch processing.\n",
    "\n",
    "\n",
    "# Define a function to compute and log text statistics for a single run.\n",
    "def compute_stats(run: Run) -> None:\n",
    "    # Check if the run has the 'input' key we want to measure.\n",
    "    if \"input\" not in run.inputs:\n",
    "        return\n",
    "    # Check if this run has already been scored to avoid redundant work.\n",
    "    if run.feedback_stats and \"smog_index\" in run.feedback_stats:\n",
    "        return\n",
    "    text = run.inputs[\"input\"] # Get the input text.\n",
    "    try:\n",
    "        # A list of readability metric functions to compute.\n",
    "        fns = [\n",
    "            \"flesch_reading_ease\",\n",
    "            \"flesch_kincaid_grade\",\n",
    "            \"smog_index\",\n",
    "            \"coleman_liau_index\",\n",
    "            \"automated_readability_index\",\n",
    "        ]\n",
    "        # Compute each metric and store it in a dictionary.\n",
    "        metrics = {fn: getattr(textstat, fn)(text) for fn in fns}\n",
    "        # Loop through the computed metrics.\n",
    "        for key, value in metrics.items():\n",
    "            # Use the client to create feedback for the original run.\n",
    "            client.create_feedback(\n",
    "                run.id, # The ID of the run to attach feedback to.\n",
    "                key=key, # The name of the metric (e.g., 'smog_index').\n",
    "                score=value,  # The numeric score, used for monitoring charts.\n",
    "                feedback_source_type=\"model\", # Specify the source as 'model' or 'auto'.\n",
    "            )\n",
    "    except Exception:\n",
    "        # Pass silently if textstat fails on a given input.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eadd5bf8-f91a-410c-b246-72b75d8d290e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap our function in a RunnableLambda and use .batch() to apply it concurrently to all runs.\n",
    "_ = RunnableLambda(compute_stats).batch(\n",
    "    runs,\n",
    "    {\"max_concurrency\": 10}, # Control the level of concurrency.\n",
    "    return_exceptions=True, # Prevent the whole batch from failing if one run errors.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b85c62-df99-4b21-a2c3-61137a3c0627",
   "metadata": {},
   "source": [
    "#### Example B: AI-Assisted Feedback\n",
    "\n",
    "While simple statistics are useful, **AI-assisted feedback** is much more powerful. Here, we'll use an LLM as a judge to score our runs on more subjective or complex criteria. This allows you to create metrics that are highly specific to your application's goals.\n",
    "\n",
    "In this example, we will create an evaluator chain that scores each user query along several axes: `relevance` (to LangChain), `difficulty`, `verbosity`, and `specificity`. We will use a pre-built prompt from the LangChain Hub and OpenAI's function-calling feature to ensure the LLM returns a structured JSON output with these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5db72c58-ae92-47bc-9f0b-6e1688c91c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub # Import the LangChain Hub client.\n",
    "\n",
    "# Pull a pre-made prompt for this task from the Hub.\n",
    "prompt = hub.pull(\n",
    "    \"wfh/automated-feedback-example\", api_url=\"https://api.hub.langchain.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccbdd724-5b1d-410c-bc0d-124800d197df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser # Import the function output parser.\n",
    "from langchain_core.tracers.context import collect_runs # Import a context manager to capture traces.\n",
    "from langchain_openai import ChatOpenAI # Import the OpenAI chat model wrapper.\n",
    "\n",
    "# Define the evaluator chain.\n",
    "chain = (\n",
    "    prompt\n",
    "    # Bind a function-calling schema to the LLM to force structured output.\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=1).bind(\n",
    "        functions=[\n",
    "            {\n",
    "                \"name\": \"submit_scores\",\n",
    "                \"description\": \"Submit the graded scores for a user question and bot response.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"relevance\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5, \"description\": \"Score indicating the relevance of the question to LangChain/LangSmith.\"},\n",
    "                        \"difficulty\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5, \"description\": \"Score indicating the complexity or difficulty of the question.\"},\n",
    "                        \"verbosity\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5, \"description\": \"Score indicating how verbose the question is.\"},\n",
    "                        \"specificity\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5, \"description\": \"Score indicating how specific the question is.\"},\n",
    "                    },\n",
    "                    \"required\": [\"relevance\", \"difficulty\", \"verbosity\", \"specificity\"],\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    | JsonOutputFunctionsParser() # Parse the LLM's function call into a JSON object.\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to evaluate a single run.\n",
    "def evaluate_run(run: Run) -> None:\n",
    "    try:\n",
    "        if \"input\" not in run.inputs or not run.outputs or \"output\" not in run.outputs:\n",
    "            return\n",
    "        if run.feedback_stats and \"specificity\" in run.feedback_stats:\n",
    "            return\n",
    "        # Use collect_runs to capture the trace of the evaluator chain itself.\n",
    "        with collect_runs() as cb:\n",
    "            result = chain.invoke(\n",
    "                {\n",
    "                    \"question\": run.inputs[\"input\"][:3000],  # Truncate to avoid context length issues.\n",
    "                    \"prediction\": run.outputs[\"output\"][:3000],\n",
    "                },\n",
    "            )\n",
    "            # Loop through the scores returned by the evaluator chain.\n",
    "            for feedback_key, value in result.items():\n",
    "                score = int(value) / 5 # Normalize the score to be between 0 and 1.\n",
    "                # Create the feedback for the original run.\n",
    "                client.create_feedback(\n",
    "                    run.id,\n",
    "                    key=feedback_key,\n",
    "                    score=score,\n",
    "                    # Link the feedback to the evaluator's run trace for auditability.\n",
    "                    source_run_id=cb.traced_runs[0].id,\n",
    "                    feedback_source_type=\"model\",\n",
    "                )\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "wrapped_function = RunnableLambda(evaluate_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55e061a9-8d0e-4474-872a-2729785027f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrently apply the AI-assisted feedback logic to all runs.\n",
    "_ = wrapped_function.batch(runs, {\"max_concurrency\": 10}, return_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "088e262f-6f82-4e0b-afee-e414cc9bd38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smog_index': {'n': 6, 'avg': 0.0, 'mode': 0, 'is_all_model': True},\\n\n",
       " 'coleman_liau_index': {'n': 6,\\n\n",
       "  'avg': 7.825,\\n\n",
       "  'mode': 3.43,\\n\n",
       "  'is_all_model': True},\\n\n",
       " 'flesch_reading_ease': {'n': 6,\\n\n",
       "  'avg': 92.79666666666667,\\n\n",
       "  'mode': 75.88,\\n\n",
       "  'is_all_model': True},\\n\n",
       " 'flesch_kincaid_grade': {'n': 6,\\n\n",
       "  'avg': 1.3,\\n\n",
       "  'mode': 2.9,\\n\n",
       "  'is_all_model': True},\\n\n",
       " 'automated_readability_index': {'n': 6,\\n\n",
       "  'avg': 9.0,\\n\n",
       "  'mode': 5.2,\\n\n",
       "  'is_all_model': True}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The project's feedback_stats are updated asynchronously.\n",
    "client.read_project(project_name=project_name).feedback_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59c106-7a98-41ef-a769-78005b6ab6c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Example C: Using LangChain Evaluators\n",
    "\n",
    "LangChain provides a number of pre-built, reference-free evaluators that you can use out-of-the-box. These can be easily integrated into a feedback pipeline. For more details on the available types, check out the [LangChain evaluation documentation](https://python.langchain.com/docs/guides/productionization/evaluation).\n",
    "\n",
    "Below, we will demonstrate this by wrapping a `criteria` evaluator in a custom `RunEvaluator`. The criterion we'll use is \"completeness\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a8e56-ad86-4e7e-809b-f86704f0cd2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional # Import typing hints.\n",
    "from langchain import evaluation, callbacks # Import LangChain evaluation components.\n",
    "from langsmith import evaluation as ls_evaluation # Import LangSmith evaluation components.\n",
    "\n",
    "\n",
    "# Define our custom evaluator class, inheriting from the base RunEvaluator.\n",
    "class CompletenessEvaluator(ls_evaluation.RunEvaluator):\n",
    "    def __init__(self):\n",
    "        # Define the criterion for the evaluator.\n",
    "        criteria_description = (\n",
    "            \"Does the answer provide sufficient and complete information\"\n",
    "            \"to fully address all aspects of the question (Y)?\"\n",
    "            \" Or does it lack important details (N)?\"\n",
    "        )\n",
    "        # Load the built-in 'criteria' evaluator with our custom criterion.\n",
    "        self.evaluator = evaluation.load_evaluator(\n",
    "            \"criteria\", criteria={\"completeness\": criteria_description}\n",
    "        )\n",
    "\n",
    "    # This is the core method that will be called for each run.\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> ls_evaluation.EvaluationResult:\n",
    "        # Safety check for required fields.\n",
    "        if (\n",
    "            not run.inputs\n",
    "            or not run.inputs.get(\"input\")\n",
    "            or not run.outputs\n",
    "            or not run.outputs.get(\"output\")\n",
    "        ):\n",
    "            return ls_evaluation.EvaluationResult(key=\"completeness\", score=None)\n",
    "        question = run.inputs[\"input\"]\n",
    "        prediction = run.outputs[\"output\"]\n",
    "        # Use collect_runs to capture the trace of the evaluator itself.\n",
    "        with callbacks.collect_runs() as cb:\n",
    "            result = self.evaluator.evaluate_strings(\n",
    "                input=question, prediction=prediction\n",
    "            )\n",
    "            run_id = cb.traced_runs[0].id\n",
    "        # Return the result, linking the feedback to the evaluator's trace.\n",
    "        return ls_evaluation.EvaluationResult(\n",
    "            key=\"completeness\", evaluator_info={\"__run\": {\"run_id\": run_id}}, **result\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd061c7-039c-4799-8eeb-2acffc082545",
   "metadata": {},
   "source": [
    "By using `collect_runs` and passing the resulting run ID to the `evaluator_info` dictionary, we create a direct link in the LangSmith UI from the feedback score on the original run to the trace of the evaluator that produced that score. This is extremely useful for auditing and debugging your feedback logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fe87cd7-af8d-4e72-a994-fbf6c35a8672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = CompletenessEvaluator() # Instantiate our completeness evaluator.\n",
    "\n",
    "# You could run this in a simple for loop:\n",
    "# for run in runs:\n",
    "#     client.evaluate_run(run, evaluator)\n",
    "\n",
    "# Or, run it concurrently for better performance.\n",
    "# The `client.evaluate_run` method handles both scoring and logging the feedback.\n",
    "wrapped_function = RunnableLambda(lambda run: client.evaluate_run(run, evaluator))\n",
    "_ = wrapped_function.batch(runs, {\"max_concurrency\": 10}, return_exceptions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66e144-141a-4d23-b7c3-3627d8ae6265",
   "metadata": {},
   "source": [
    "Check out your project in LangSmith again to see the new \"completeness\" feedback scores appear on your runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb41ef-3892-4385-8830-c6decfbf8f5c",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "Post-hoc evaluation cannot prevent failures in real time and depends on the quality of logged data. It also risks missing transient behaviours that are not captured in traces.\n",
    "\n",
    "### Role in a Broader Evaluation Framework\n",
    "Within this project, algorithmic feedback provides the backbone for large-scale analysis, complementing real-time monitoring and targeted evaluations. Together, these methods support both immediate oversight and longer-term research into system behaviour.\n",
    "\n",
    "## Discussion\n",
    "As systems grow more complex and produce more data, scalable evaluation becomes essential. This notebook demonstrates how algorithmic feedback pipelines can turn raw execution traces into structured evidence for understanding, debugging, and improving deployed models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
